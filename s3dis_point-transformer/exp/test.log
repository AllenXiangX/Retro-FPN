[2023-11-04 15:54:19,752 INFO test.py line 56 3244390] arch: retro_fpn
base_lr: 0.1
batch_size: 4
batch_size_test: 4
batch_size_val: 4
classes: 13
data_name: s3dis
data_root: /data1/xp/data/s3dis/trainval_fullarea
dist_backend: nccl
dist_url: tcp://localhost:8888
drop_rate: 0.5
epochs: 100
eval_freq: 1
evaluate: True
fea_dim: 6
ignore_label: 255
loop: 10
manual_seed: 7777
model_path: ./checkpoints/model_best.pth
momentum: 0.9
multiplier: 0.1
multiprocessing_distributed: True
n_samples_retro: 16
names_path: data/s3dis/s3dis_names.txt
print_freq: 1
rank: 0
resume: None
save_folder: exp
save_freq: 1
save_path: exp/retro_fpn
split: val
start_epoch: 0
step_epoch: 30
sync_bn: False
test_area: 5
test_gpu: [7]
test_list: dataset/s3dis/list/val5.txt
test_list_full: dataset/s3dis/list/val5_full.txt
test_workers: 4
train_gpu: [0]
use_xyz: True
voxel_max: 80000
voxel_size: 0.04
weight: None
weight_decay: 0.0001
workers: 4
world_size: 1
[2023-11-04 15:54:19,752 INFO test.py line 58 3244390] => creating model ...
[2023-11-04 15:54:19,752 INFO test.py line 59 3244390] Classes: 13
[2023-11-04 15:54:23,977 INFO test.py line 68 3244390] RetroFPNSeg(
  (enc1): Sequential(
    (0): TransitionDown(
      (linear): Linear(in_features=6, out_features=32, bias=False)
      (bn): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (1): PointTransformerBlock(
      (linear1): Linear(in_features=32, out_features=32, bias=False)
      (bn1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (transformer2): PointTransformerLayer(
        (linear_q): Linear(in_features=32, out_features=32, bias=True)
        (linear_k): Linear(in_features=32, out_features=32, bias=True)
        (linear_v): Linear(in_features=32, out_features=32, bias=True)
        (linear_p): Sequential(
          (0): Conv1d(3, 3, kernel_size=(1,), stride=(1,))
          (1): BatchNorm1d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv1d(3, 32, kernel_size=(1,), stride=(1,))
        )
        (linear_w): Sequential(
          (0): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1): ReLU(inplace=True)
          (2): Conv1d(32, 4, kernel_size=(1,), stride=(1,))
          (3): BatchNorm1d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (4): ReLU(inplace=True)
          (5): Conv1d(4, 4, kernel_size=(1,), stride=(1,))
        )
        (softmax): Softmax(dim=1)
      )
      (bn2): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (linear3): Linear(in_features=32, out_features=32, bias=False)
      (bn3): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (enc2): Sequential(
    (0): TransitionDown(
      (linear): Linear(in_features=35, out_features=64, bias=False)
      (pool): MaxPool1d(kernel_size=16, stride=16, padding=0, dilation=1, ceil_mode=False)
      (bn): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (1): PointTransformerBlock(
      (linear1): Linear(in_features=64, out_features=64, bias=False)
      (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (transformer2): PointTransformerLayer(
        (linear_q): Linear(in_features=64, out_features=64, bias=True)
        (linear_k): Linear(in_features=64, out_features=64, bias=True)
        (linear_v): Linear(in_features=64, out_features=64, bias=True)
        (linear_p): Sequential(
          (0): Conv1d(3, 3, kernel_size=(1,), stride=(1,))
          (1): BatchNorm1d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv1d(3, 64, kernel_size=(1,), stride=(1,))
        )
        (linear_w): Sequential(
          (0): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1): ReLU(inplace=True)
          (2): Conv1d(64, 8, kernel_size=(1,), stride=(1,))
          (3): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (4): ReLU(inplace=True)
          (5): Conv1d(8, 8, kernel_size=(1,), stride=(1,))
        )
        (softmax): Softmax(dim=1)
      )
      (bn2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (linear3): Linear(in_features=64, out_features=64, bias=False)
      (bn3): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): PointTransformerBlock(
      (linear1): Linear(in_features=64, out_features=64, bias=False)
      (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (transformer2): PointTransformerLayer(
        (linear_q): Linear(in_features=64, out_features=64, bias=True)
        (linear_k): Linear(in_features=64, out_features=64, bias=True)
        (linear_v): Linear(in_features=64, out_features=64, bias=True)
        (linear_p): Sequential(
          (0): Conv1d(3, 3, kernel_size=(1,), stride=(1,))
          (1): BatchNorm1d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv1d(3, 64, kernel_size=(1,), stride=(1,))
        )
        (linear_w): Sequential(
          (0): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1): ReLU(inplace=True)
          (2): Conv1d(64, 8, kernel_size=(1,), stride=(1,))
          (3): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (4): ReLU(inplace=True)
          (5): Conv1d(8, 8, kernel_size=(1,), stride=(1,))
        )
        (softmax): Softmax(dim=1)
      )
      (bn2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (linear3): Linear(in_features=64, out_features=64, bias=False)
      (bn3): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (enc3): Sequential(
    (0): TransitionDown(
      (linear): Linear(in_features=67, out_features=128, bias=False)
      (pool): MaxPool1d(kernel_size=16, stride=16, padding=0, dilation=1, ceil_mode=False)
      (bn): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (1): PointTransformerBlock(
      (linear1): Linear(in_features=128, out_features=128, bias=False)
      (bn1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (transformer2): PointTransformerLayer(
        (linear_q): Linear(in_features=128, out_features=128, bias=True)
        (linear_k): Linear(in_features=128, out_features=128, bias=True)
        (linear_v): Linear(in_features=128, out_features=128, bias=True)
        (linear_p): Sequential(
          (0): Conv1d(3, 3, kernel_size=(1,), stride=(1,))
          (1): BatchNorm1d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv1d(3, 128, kernel_size=(1,), stride=(1,))
        )
        (linear_w): Sequential(
          (0): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1): ReLU(inplace=True)
          (2): Conv1d(128, 16, kernel_size=(1,), stride=(1,))
          (3): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (4): ReLU(inplace=True)
          (5): Conv1d(16, 16, kernel_size=(1,), stride=(1,))
        )
        (softmax): Softmax(dim=1)
      )
      (bn2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (linear3): Linear(in_features=128, out_features=128, bias=False)
      (bn3): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): PointTransformerBlock(
      (linear1): Linear(in_features=128, out_features=128, bias=False)
      (bn1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (transformer2): PointTransformerLayer(
        (linear_q): Linear(in_features=128, out_features=128, bias=True)
        (linear_k): Linear(in_features=128, out_features=128, bias=True)
        (linear_v): Linear(in_features=128, out_features=128, bias=True)
        (linear_p): Sequential(
          (0): Conv1d(3, 3, kernel_size=(1,), stride=(1,))
          (1): BatchNorm1d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv1d(3, 128, kernel_size=(1,), stride=(1,))
        )
        (linear_w): Sequential(
          (0): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1): ReLU(inplace=True)
          (2): Conv1d(128, 16, kernel_size=(1,), stride=(1,))
          (3): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (4): ReLU(inplace=True)
          (5): Conv1d(16, 16, kernel_size=(1,), stride=(1,))
        )
        (softmax): Softmax(dim=1)
      )
      (bn2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (linear3): Linear(in_features=128, out_features=128, bias=False)
      (bn3): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (3): PointTransformerBlock(
      (linear1): Linear(in_features=128, out_features=128, bias=False)
      (bn1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (transformer2): PointTransformerLayer(
        (linear_q): Linear(in_features=128, out_features=128, bias=True)
        (linear_k): Linear(in_features=128, out_features=128, bias=True)
        (linear_v): Linear(in_features=128, out_features=128, bias=True)
        (linear_p): Sequential(
          (0): Conv1d(3, 3, kernel_size=(1,), stride=(1,))
          (1): BatchNorm1d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv1d(3, 128, kernel_size=(1,), stride=(1,))
        )
        (linear_w): Sequential(
          (0): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1): ReLU(inplace=True)
          (2): Conv1d(128, 16, kernel_size=(1,), stride=(1,))
          (3): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (4): ReLU(inplace=True)
          (5): Conv1d(16, 16, kernel_size=(1,), stride=(1,))
        )
        (softmax): Softmax(dim=1)
      )
      (bn2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (linear3): Linear(in_features=128, out_features=128, bias=False)
      (bn3): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (enc4): Sequential(
    (0): TransitionDown(
      (linear): Linear(in_features=131, out_features=256, bias=False)
      (pool): MaxPool1d(kernel_size=16, stride=16, padding=0, dilation=1, ceil_mode=False)
      (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (1): PointTransformerBlock(
      (linear1): Linear(in_features=256, out_features=256, bias=False)
      (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (transformer2): PointTransformerLayer(
        (linear_q): Linear(in_features=256, out_features=256, bias=True)
        (linear_k): Linear(in_features=256, out_features=256, bias=True)
        (linear_v): Linear(in_features=256, out_features=256, bias=True)
        (linear_p): Sequential(
          (0): Conv1d(3, 3, kernel_size=(1,), stride=(1,))
          (1): BatchNorm1d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv1d(3, 256, kernel_size=(1,), stride=(1,))
        )
        (linear_w): Sequential(
          (0): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1): ReLU(inplace=True)
          (2): Conv1d(256, 32, kernel_size=(1,), stride=(1,))
          (3): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (4): ReLU(inplace=True)
          (5): Conv1d(32, 32, kernel_size=(1,), stride=(1,))
        )
        (softmax): Softmax(dim=1)
      )
      (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (linear3): Linear(in_features=256, out_features=256, bias=False)
      (bn3): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): PointTransformerBlock(
      (linear1): Linear(in_features=256, out_features=256, bias=False)
      (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (transformer2): PointTransformerLayer(
        (linear_q): Linear(in_features=256, out_features=256, bias=True)
        (linear_k): Linear(in_features=256, out_features=256, bias=True)
        (linear_v): Linear(in_features=256, out_features=256, bias=True)
        (linear_p): Sequential(
          (0): Conv1d(3, 3, kernel_size=(1,), stride=(1,))
          (1): BatchNorm1d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv1d(3, 256, kernel_size=(1,), stride=(1,))
        )
        (linear_w): Sequential(
          (0): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1): ReLU(inplace=True)
          (2): Conv1d(256, 32, kernel_size=(1,), stride=(1,))
          (3): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (4): ReLU(inplace=True)
          (5): Conv1d(32, 32, kernel_size=(1,), stride=(1,))
        )
        (softmax): Softmax(dim=1)
      )
      (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (linear3): Linear(in_features=256, out_features=256, bias=False)
      (bn3): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (3): PointTransformerBlock(
      (linear1): Linear(in_features=256, out_features=256, bias=False)
      (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (transformer2): PointTransformerLayer(
        (linear_q): Linear(in_features=256, out_features=256, bias=True)
        (linear_k): Linear(in_features=256, out_features=256, bias=True)
        (linear_v): Linear(in_features=256, out_features=256, bias=True)
        (linear_p): Sequential(
          (0): Conv1d(3, 3, kernel_size=(1,), stride=(1,))
          (1): BatchNorm1d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv1d(3, 256, kernel_size=(1,), stride=(1,))
        )
        (linear_w): Sequential(
          (0): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1): ReLU(inplace=True)
          (2): Conv1d(256, 32, kernel_size=(1,), stride=(1,))
          (3): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (4): ReLU(inplace=True)
          (5): Conv1d(32, 32, kernel_size=(1,), stride=(1,))
        )
        (softmax): Softmax(dim=1)
      )
      (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (linear3): Linear(in_features=256, out_features=256, bias=False)
      (bn3): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (4): PointTransformerBlock(
      (linear1): Linear(in_features=256, out_features=256, bias=False)
      (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (transformer2): PointTransformerLayer(
        (linear_q): Linear(in_features=256, out_features=256, bias=True)
        (linear_k): Linear(in_features=256, out_features=256, bias=True)
        (linear_v): Linear(in_features=256, out_features=256, bias=True)
        (linear_p): Sequential(
          (0): Conv1d(3, 3, kernel_size=(1,), stride=(1,))
          (1): BatchNorm1d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv1d(3, 256, kernel_size=(1,), stride=(1,))
        )
        (linear_w): Sequential(
          (0): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1): ReLU(inplace=True)
          (2): Conv1d(256, 32, kernel_size=(1,), stride=(1,))
          (3): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (4): ReLU(inplace=True)
          (5): Conv1d(32, 32, kernel_size=(1,), stride=(1,))
        )
        (softmax): Softmax(dim=1)
      )
      (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (linear3): Linear(in_features=256, out_features=256, bias=False)
      (bn3): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (5): PointTransformerBlock(
      (linear1): Linear(in_features=256, out_features=256, bias=False)
      (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (transformer2): PointTransformerLayer(
        (linear_q): Linear(in_features=256, out_features=256, bias=True)
        (linear_k): Linear(in_features=256, out_features=256, bias=True)
        (linear_v): Linear(in_features=256, out_features=256, bias=True)
        (linear_p): Sequential(
          (0): Conv1d(3, 3, kernel_size=(1,), stride=(1,))
          (1): BatchNorm1d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv1d(3, 256, kernel_size=(1,), stride=(1,))
        )
        (linear_w): Sequential(
          (0): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1): ReLU(inplace=True)
          (2): Conv1d(256, 32, kernel_size=(1,), stride=(1,))
          (3): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (4): ReLU(inplace=True)
          (5): Conv1d(32, 32, kernel_size=(1,), stride=(1,))
        )
        (softmax): Softmax(dim=1)
      )
      (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (linear3): Linear(in_features=256, out_features=256, bias=False)
      (bn3): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (enc5): Sequential(
    (0): TransitionDown(
      (linear): Linear(in_features=259, out_features=512, bias=False)
      (pool): MaxPool1d(kernel_size=16, stride=16, padding=0, dilation=1, ceil_mode=False)
      (bn): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (1): PointTransformerBlock(
      (linear1): Linear(in_features=512, out_features=512, bias=False)
      (bn1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (transformer2): PointTransformerLayer(
        (linear_q): Linear(in_features=512, out_features=512, bias=True)
        (linear_k): Linear(in_features=512, out_features=512, bias=True)
        (linear_v): Linear(in_features=512, out_features=512, bias=True)
        (linear_p): Sequential(
          (0): Conv1d(3, 3, kernel_size=(1,), stride=(1,))
          (1): BatchNorm1d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv1d(3, 512, kernel_size=(1,), stride=(1,))
        )
        (linear_w): Sequential(
          (0): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1): ReLU(inplace=True)
          (2): Conv1d(512, 64, kernel_size=(1,), stride=(1,))
          (3): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (4): ReLU(inplace=True)
          (5): Conv1d(64, 64, kernel_size=(1,), stride=(1,))
        )
        (softmax): Softmax(dim=1)
      )
      (bn2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (linear3): Linear(in_features=512, out_features=512, bias=False)
      (bn3): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): PointTransformerBlock(
      (linear1): Linear(in_features=512, out_features=512, bias=False)
      (bn1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (transformer2): PointTransformerLayer(
        (linear_q): Linear(in_features=512, out_features=512, bias=True)
        (linear_k): Linear(in_features=512, out_features=512, bias=True)
        (linear_v): Linear(in_features=512, out_features=512, bias=True)
        (linear_p): Sequential(
          (0): Conv1d(3, 3, kernel_size=(1,), stride=(1,))
          (1): BatchNorm1d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv1d(3, 512, kernel_size=(1,), stride=(1,))
        )
        (linear_w): Sequential(
          (0): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1): ReLU(inplace=True)
          (2): Conv1d(512, 64, kernel_size=(1,), stride=(1,))
          (3): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (4): ReLU(inplace=True)
          (5): Conv1d(64, 64, kernel_size=(1,), stride=(1,))
        )
        (softmax): Softmax(dim=1)
      )
      (bn2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (linear3): Linear(in_features=512, out_features=512, bias=False)
      (bn3): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (dec5): Sequential(
    (0): TransitionUp(
      (linear1): Sequential(
        (0): Linear(in_features=1024, out_features=512, bias=True)
        (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
      (linear2): Sequential(
        (0): Linear(in_features=512, out_features=512, bias=True)
        (1): ReLU(inplace=True)
      )
    )
    (1): PointTransformerBlock(
      (linear1): Linear(in_features=512, out_features=512, bias=False)
      (bn1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (transformer2): PointTransformerLayer(
        (linear_q): Linear(in_features=512, out_features=512, bias=True)
        (linear_k): Linear(in_features=512, out_features=512, bias=True)
        (linear_v): Linear(in_features=512, out_features=512, bias=True)
        (linear_p): Sequential(
          (0): Conv1d(3, 3, kernel_size=(1,), stride=(1,))
          (1): BatchNorm1d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv1d(3, 512, kernel_size=(1,), stride=(1,))
        )
        (linear_w): Sequential(
          (0): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1): ReLU(inplace=True)
          (2): Conv1d(512, 64, kernel_size=(1,), stride=(1,))
          (3): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (4): ReLU(inplace=True)
          (5): Conv1d(64, 64, kernel_size=(1,), stride=(1,))
        )
        (softmax): Softmax(dim=1)
      )
      (bn2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (linear3): Linear(in_features=512, out_features=512, bias=False)
      (bn3): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (rt5): RetroTransformer(
    (mlp_bottleneck): Sequential(
      (0): Linear(in_features=512, out_features=32, bias=False)
      (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Linear(in_features=32, out_features=32, bias=False)
    )
    (cross_attention): CrossAttention(
      (linear_q): Linear(in_features=32, out_features=32, bias=True)
      (linear_k): Linear(in_features=32, out_features=32, bias=True)
      (linear_v): Linear(in_features=32, out_features=32, bias=True)
      (linear_p): Sequential(
        (0): Conv1d(3, 3, kernel_size=(1,), stride=(1,))
        (1): BatchNorm1d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
        (3): Conv1d(3, 2, kernel_size=(1,), stride=(1,))
      )
      (softmax): Softmax(dim=1)
    )
    (mlp_out): Sequential(
      (0): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): ReLU(inplace=True)
      (2): Linear(in_features=32, out_features=32, bias=False)
      (3): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (linear_info_curr): Linear(in_features=512, out_features=32, bias=True)
    (mlp_gate): Sequential(
      (0): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): Linear(in_features=32, out_features=32, bias=False)
      (2): Sigmoid()
    )
  )
  (cls5): Sequential(
    (0): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (1): ReLU()
    (2): Linear(in_features=32, out_features=13, bias=True)
  )
  (dec4): Sequential(
    (0): TransitionUp(
      (linear1): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
      (linear2): Sequential(
        (0): Linear(in_features=512, out_features=256, bias=True)
        (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
    )
    (1): PointTransformerBlock(
      (linear1): Linear(in_features=256, out_features=256, bias=False)
      (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (transformer2): PointTransformerLayer(
        (linear_q): Linear(in_features=256, out_features=256, bias=True)
        (linear_k): Linear(in_features=256, out_features=256, bias=True)
        (linear_v): Linear(in_features=256, out_features=256, bias=True)
        (linear_p): Sequential(
          (0): Conv1d(3, 3, kernel_size=(1,), stride=(1,))
          (1): BatchNorm1d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv1d(3, 256, kernel_size=(1,), stride=(1,))
        )
        (linear_w): Sequential(
          (0): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1): ReLU(inplace=True)
          (2): Conv1d(256, 32, kernel_size=(1,), stride=(1,))
          (3): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (4): ReLU(inplace=True)
          (5): Conv1d(32, 32, kernel_size=(1,), stride=(1,))
        )
        (softmax): Softmax(dim=1)
      )
      (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (linear3): Linear(in_features=256, out_features=256, bias=False)
      (bn3): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (rt4): RetroTransformer(
    (mlp_bottleneck): Sequential(
      (0): Linear(in_features=256, out_features=32, bias=False)
      (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Linear(in_features=32, out_features=32, bias=False)
    )
    (cross_attention): CrossAttention(
      (linear_q): Linear(in_features=32, out_features=32, bias=True)
      (linear_k): Linear(in_features=32, out_features=32, bias=True)
      (linear_v): Linear(in_features=32, out_features=32, bias=True)
      (linear_p): Sequential(
        (0): Conv1d(3, 3, kernel_size=(1,), stride=(1,))
        (1): BatchNorm1d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
        (3): Conv1d(3, 2, kernel_size=(1,), stride=(1,))
      )
      (softmax): Softmax(dim=1)
    )
    (mlp_out): Sequential(
      (0): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): ReLU(inplace=True)
      (2): Linear(in_features=32, out_features=32, bias=False)
      (3): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (linear_info_curr): Linear(in_features=256, out_features=32, bias=True)
    (mlp_gate): Sequential(
      (0): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): Linear(in_features=32, out_features=32, bias=False)
      (2): Sigmoid()
    )
  )
  (cls4): Sequential(
    (0): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (1): ReLU()
    (2): Linear(in_features=32, out_features=13, bias=True)
  )
  (dec3): Sequential(
    (0): TransitionUp(
      (linear1): Sequential(
        (0): Linear(in_features=128, out_features=128, bias=True)
        (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
      (linear2): Sequential(
        (0): Linear(in_features=256, out_features=128, bias=True)
        (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
    )
    (1): PointTransformerBlock(
      (linear1): Linear(in_features=128, out_features=128, bias=False)
      (bn1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (transformer2): PointTransformerLayer(
        (linear_q): Linear(in_features=128, out_features=128, bias=True)
        (linear_k): Linear(in_features=128, out_features=128, bias=True)
        (linear_v): Linear(in_features=128, out_features=128, bias=True)
        (linear_p): Sequential(
          (0): Conv1d(3, 3, kernel_size=(1,), stride=(1,))
          (1): BatchNorm1d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv1d(3, 128, kernel_size=(1,), stride=(1,))
        )
        (linear_w): Sequential(
          (0): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1): ReLU(inplace=True)
          (2): Conv1d(128, 16, kernel_size=(1,), stride=(1,))
          (3): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (4): ReLU(inplace=True)
          (5): Conv1d(16, 16, kernel_size=(1,), stride=(1,))
        )
        (softmax): Softmax(dim=1)
      )
      (bn2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (linear3): Linear(in_features=128, out_features=128, bias=False)
      (bn3): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (rt3): RetroTransformer(
    (mlp_bottleneck): Sequential(
      (0): Linear(in_features=128, out_features=32, bias=False)
      (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Linear(in_features=32, out_features=32, bias=False)
    )
    (cross_attention): CrossAttention(
      (linear_q): Linear(in_features=32, out_features=32, bias=True)
      (linear_k): Linear(in_features=32, out_features=32, bias=True)
      (linear_v): Linear(in_features=32, out_features=32, bias=True)
      (linear_p): Sequential(
        (0): Conv1d(3, 3, kernel_size=(1,), stride=(1,))
        (1): BatchNorm1d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
        (3): Conv1d(3, 2, kernel_size=(1,), stride=(1,))
      )
      (softmax): Softmax(dim=1)
    )
    (mlp_out): Sequential(
      (0): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): ReLU(inplace=True)
      (2): Linear(in_features=32, out_features=32, bias=False)
      (3): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (linear_info_curr): Linear(in_features=128, out_features=32, bias=True)
    (mlp_gate): Sequential(
      (0): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): Linear(in_features=32, out_features=32, bias=False)
      (2): Sigmoid()
    )
  )
  (cls3): Sequential(
    (0): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (1): ReLU()
    (2): Linear(in_features=32, out_features=13, bias=True)
  )
  (dec2): Sequential(
    (0): TransitionUp(
      (linear1): Sequential(
        (0): Linear(in_features=64, out_features=64, bias=True)
        (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
      (linear2): Sequential(
        (0): Linear(in_features=128, out_features=64, bias=True)
        (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
    )
    (1): PointTransformerBlock(
      (linear1): Linear(in_features=64, out_features=64, bias=False)
      (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (transformer2): PointTransformerLayer(
        (linear_q): Linear(in_features=64, out_features=64, bias=True)
        (linear_k): Linear(in_features=64, out_features=64, bias=True)
        (linear_v): Linear(in_features=64, out_features=64, bias=True)
        (linear_p): Sequential(
          (0): Conv1d(3, 3, kernel_size=(1,), stride=(1,))
          (1): BatchNorm1d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv1d(3, 64, kernel_size=(1,), stride=(1,))
        )
        (linear_w): Sequential(
          (0): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1): ReLU(inplace=True)
          (2): Conv1d(64, 8, kernel_size=(1,), stride=(1,))
          (3): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (4): ReLU(inplace=True)
          (5): Conv1d(8, 8, kernel_size=(1,), stride=(1,))
        )
        (softmax): Softmax(dim=1)
      )
      (bn2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (linear3): Linear(in_features=64, out_features=64, bias=False)
      (bn3): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (rt2): RetroTransformer(
    (mlp_bottleneck): Sequential(
      (0): Linear(in_features=64, out_features=32, bias=False)
      (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Linear(in_features=32, out_features=32, bias=False)
    )
    (cross_attention): CrossAttention(
      (linear_q): Linear(in_features=32, out_features=32, bias=True)
      (linear_k): Linear(in_features=32, out_features=32, bias=True)
      (linear_v): Linear(in_features=32, out_features=32, bias=True)
      (linear_p): Sequential(
        (0): Conv1d(3, 3, kernel_size=(1,), stride=(1,))
        (1): BatchNorm1d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
        (3): Conv1d(3, 2, kernel_size=(1,), stride=(1,))
      )
      (softmax): Softmax(dim=1)
    )
    (mlp_out): Sequential(
      (0): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): ReLU(inplace=True)
      (2): Linear(in_features=32, out_features=32, bias=False)
      (3): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (linear_info_curr): Linear(in_features=64, out_features=32, bias=True)
    (mlp_gate): Sequential(
      (0): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): Linear(in_features=32, out_features=32, bias=False)
      (2): Sigmoid()
    )
  )
  (cls2): Sequential(
    (0): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (1): ReLU()
    (2): Linear(in_features=32, out_features=13, bias=True)
  )
  (dec1): Sequential(
    (0): TransitionUp(
      (linear1): Sequential(
        (0): Linear(in_features=32, out_features=32, bias=True)
        (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
      (linear2): Sequential(
        (0): Linear(in_features=64, out_features=32, bias=True)
        (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
    )
    (1): PointTransformerBlock(
      (linear1): Linear(in_features=32, out_features=32, bias=False)
      (bn1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (transformer2): PointTransformerLayer(
        (linear_q): Linear(in_features=32, out_features=32, bias=True)
        (linear_k): Linear(in_features=32, out_features=32, bias=True)
        (linear_v): Linear(in_features=32, out_features=32, bias=True)
        (linear_p): Sequential(
          (0): Conv1d(3, 3, kernel_size=(1,), stride=(1,))
          (1): BatchNorm1d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv1d(3, 32, kernel_size=(1,), stride=(1,))
        )
        (linear_w): Sequential(
          (0): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1): ReLU(inplace=True)
          (2): Conv1d(32, 4, kernel_size=(1,), stride=(1,))
          (3): BatchNorm1d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (4): ReLU(inplace=True)
          (5): Conv1d(4, 4, kernel_size=(1,), stride=(1,))
        )
        (softmax): Softmax(dim=1)
      )
      (bn2): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (linear3): Linear(in_features=32, out_features=32, bias=False)
      (bn3): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (rt1): RetroTransformer(
    (mlp_bottleneck): Sequential(
      (0): Linear(in_features=32, out_features=32, bias=False)
      (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Linear(in_features=32, out_features=32, bias=False)
    )
    (cross_attention): CrossAttention(
      (linear_q): Linear(in_features=32, out_features=32, bias=True)
      (linear_k): Linear(in_features=32, out_features=32, bias=True)
      (linear_v): Linear(in_features=32, out_features=32, bias=True)
      (linear_p): Sequential(
        (0): Conv1d(3, 3, kernel_size=(1,), stride=(1,))
        (1): BatchNorm1d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
        (3): Conv1d(3, 2, kernel_size=(1,), stride=(1,))
      )
      (softmax): Softmax(dim=1)
    )
    (mlp_out): Sequential(
      (0): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): ReLU(inplace=True)
      (2): Linear(in_features=32, out_features=32, bias=False)
      (3): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (linear_info_curr): Linear(in_features=32, out_features=32, bias=True)
    (mlp_gate): Sequential(
      (0): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): Linear(in_features=32, out_features=32, bias=False)
      (2): Sigmoid()
    )
  )
  (cls1): Sequential(
    (0): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (1): ReLU()
    (2): Linear(in_features=32, out_features=13, bias=True)
  )
)
[2023-11-04 15:54:24,208 INFO test.py line 72 3244390] => loading checkpoint './checkpoints/model_best.pth'
[2023-11-04 15:56:02,309 INFO test.py line 56 3258548] arch: retro_fpn
base_lr: 0.1
batch_size: 4
batch_size_test: 4
batch_size_val: 4
classes: 13
data_name: s3dis
data_root: /data1/xp/data/s3dis/trainval_fullarea
dist_backend: nccl
dist_url: tcp://localhost:8888
drop_rate: 0.5
epochs: 100
eval_freq: 1
evaluate: True
fea_dim: 6
ignore_label: 255
loop: 10
manual_seed: 7777
model_path: ./checkpoints/model_best.pth
momentum: 0.9
multiplier: 0.1
multiprocessing_distributed: True
n_samples_retro: 16
names_path: data/s3dis/s3dis_names.txt
print_freq: 1
rank: 0
resume: None
save_folder: exp
save_freq: 1
save_path: exp/retro_fpn
split: val
start_epoch: 0
step_epoch: 30
sync_bn: False
test_area: 5
test_gpu: [7]
test_list: dataset/s3dis/list/val5.txt
test_list_full: dataset/s3dis/list/val5_full.txt
test_workers: 4
train_gpu: [0]
use_xyz: True
voxel_max: 80000
voxel_size: 0.04
weight: None
weight_decay: 0.0001
workers: 4
world_size: 1
[2023-11-04 15:56:02,309 INFO test.py line 58 3258548] => creating model ...
[2023-11-04 15:56:02,310 INFO test.py line 59 3258548] Classes: 13
[2023-11-04 15:56:03,708 INFO test.py line 68 3258548] RetroFPNSeg(
  (enc1): Sequential(
    (0): TransitionDown(
      (linear): Linear(in_features=6, out_features=32, bias=False)
      (bn): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (1): PointTransformerBlock(
      (linear1): Linear(in_features=32, out_features=32, bias=False)
      (bn1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (transformer2): PointTransformerLayer(
        (linear_q): Linear(in_features=32, out_features=32, bias=True)
        (linear_k): Linear(in_features=32, out_features=32, bias=True)
        (linear_v): Linear(in_features=32, out_features=32, bias=True)
        (linear_p): Sequential(
          (0): Conv1d(3, 3, kernel_size=(1,), stride=(1,))
          (1): BatchNorm1d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv1d(3, 32, kernel_size=(1,), stride=(1,))
        )
        (linear_w): Sequential(
          (0): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1): ReLU(inplace=True)
          (2): Conv1d(32, 4, kernel_size=(1,), stride=(1,))
          (3): BatchNorm1d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (4): ReLU(inplace=True)
          (5): Conv1d(4, 4, kernel_size=(1,), stride=(1,))
        )
        (softmax): Softmax(dim=1)
      )
      (bn2): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (linear3): Linear(in_features=32, out_features=32, bias=False)
      (bn3): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (enc2): Sequential(
    (0): TransitionDown(
      (linear): Linear(in_features=35, out_features=64, bias=False)
      (pool): MaxPool1d(kernel_size=16, stride=16, padding=0, dilation=1, ceil_mode=False)
      (bn): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (1): PointTransformerBlock(
      (linear1): Linear(in_features=64, out_features=64, bias=False)
      (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (transformer2): PointTransformerLayer(
        (linear_q): Linear(in_features=64, out_features=64, bias=True)
        (linear_k): Linear(in_features=64, out_features=64, bias=True)
        (linear_v): Linear(in_features=64, out_features=64, bias=True)
        (linear_p): Sequential(
          (0): Conv1d(3, 3, kernel_size=(1,), stride=(1,))
          (1): BatchNorm1d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv1d(3, 64, kernel_size=(1,), stride=(1,))
        )
        (linear_w): Sequential(
          (0): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1): ReLU(inplace=True)
          (2): Conv1d(64, 8, kernel_size=(1,), stride=(1,))
          (3): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (4): ReLU(inplace=True)
          (5): Conv1d(8, 8, kernel_size=(1,), stride=(1,))
        )
        (softmax): Softmax(dim=1)
      )
      (bn2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (linear3): Linear(in_features=64, out_features=64, bias=False)
      (bn3): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): PointTransformerBlock(
      (linear1): Linear(in_features=64, out_features=64, bias=False)
      (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (transformer2): PointTransformerLayer(
        (linear_q): Linear(in_features=64, out_features=64, bias=True)
        (linear_k): Linear(in_features=64, out_features=64, bias=True)
        (linear_v): Linear(in_features=64, out_features=64, bias=True)
        (linear_p): Sequential(
          (0): Conv1d(3, 3, kernel_size=(1,), stride=(1,))
          (1): BatchNorm1d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv1d(3, 64, kernel_size=(1,), stride=(1,))
        )
        (linear_w): Sequential(
          (0): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1): ReLU(inplace=True)
          (2): Conv1d(64, 8, kernel_size=(1,), stride=(1,))
          (3): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (4): ReLU(inplace=True)
          (5): Conv1d(8, 8, kernel_size=(1,), stride=(1,))
        )
        (softmax): Softmax(dim=1)
      )
      (bn2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (linear3): Linear(in_features=64, out_features=64, bias=False)
      (bn3): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (enc3): Sequential(
    (0): TransitionDown(
      (linear): Linear(in_features=67, out_features=128, bias=False)
      (pool): MaxPool1d(kernel_size=16, stride=16, padding=0, dilation=1, ceil_mode=False)
      (bn): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (1): PointTransformerBlock(
      (linear1): Linear(in_features=128, out_features=128, bias=False)
      (bn1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (transformer2): PointTransformerLayer(
        (linear_q): Linear(in_features=128, out_features=128, bias=True)
        (linear_k): Linear(in_features=128, out_features=128, bias=True)
        (linear_v): Linear(in_features=128, out_features=128, bias=True)
        (linear_p): Sequential(
          (0): Conv1d(3, 3, kernel_size=(1,), stride=(1,))
          (1): BatchNorm1d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv1d(3, 128, kernel_size=(1,), stride=(1,))
        )
        (linear_w): Sequential(
          (0): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1): ReLU(inplace=True)
          (2): Conv1d(128, 16, kernel_size=(1,), stride=(1,))
          (3): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (4): ReLU(inplace=True)
          (5): Conv1d(16, 16, kernel_size=(1,), stride=(1,))
        )
        (softmax): Softmax(dim=1)
      )
      (bn2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (linear3): Linear(in_features=128, out_features=128, bias=False)
      (bn3): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): PointTransformerBlock(
      (linear1): Linear(in_features=128, out_features=128, bias=False)
      (bn1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (transformer2): PointTransformerLayer(
        (linear_q): Linear(in_features=128, out_features=128, bias=True)
        (linear_k): Linear(in_features=128, out_features=128, bias=True)
        (linear_v): Linear(in_features=128, out_features=128, bias=True)
        (linear_p): Sequential(
          (0): Conv1d(3, 3, kernel_size=(1,), stride=(1,))
          (1): BatchNorm1d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv1d(3, 128, kernel_size=(1,), stride=(1,))
        )
        (linear_w): Sequential(
          (0): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1): ReLU(inplace=True)
          (2): Conv1d(128, 16, kernel_size=(1,), stride=(1,))
          (3): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (4): ReLU(inplace=True)
          (5): Conv1d(16, 16, kernel_size=(1,), stride=(1,))
        )
        (softmax): Softmax(dim=1)
      )
      (bn2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (linear3): Linear(in_features=128, out_features=128, bias=False)
      (bn3): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (3): PointTransformerBlock(
      (linear1): Linear(in_features=128, out_features=128, bias=False)
      (bn1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (transformer2): PointTransformerLayer(
        (linear_q): Linear(in_features=128, out_features=128, bias=True)
        (linear_k): Linear(in_features=128, out_features=128, bias=True)
        (linear_v): Linear(in_features=128, out_features=128, bias=True)
        (linear_p): Sequential(
          (0): Conv1d(3, 3, kernel_size=(1,), stride=(1,))
          (1): BatchNorm1d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv1d(3, 128, kernel_size=(1,), stride=(1,))
        )
        (linear_w): Sequential(
          (0): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1): ReLU(inplace=True)
          (2): Conv1d(128, 16, kernel_size=(1,), stride=(1,))
          (3): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (4): ReLU(inplace=True)
          (5): Conv1d(16, 16, kernel_size=(1,), stride=(1,))
        )
        (softmax): Softmax(dim=1)
      )
      (bn2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (linear3): Linear(in_features=128, out_features=128, bias=False)
      (bn3): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (enc4): Sequential(
    (0): TransitionDown(
      (linear): Linear(in_features=131, out_features=256, bias=False)
      (pool): MaxPool1d(kernel_size=16, stride=16, padding=0, dilation=1, ceil_mode=False)
      (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (1): PointTransformerBlock(
      (linear1): Linear(in_features=256, out_features=256, bias=False)
      (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (transformer2): PointTransformerLayer(
        (linear_q): Linear(in_features=256, out_features=256, bias=True)
        (linear_k): Linear(in_features=256, out_features=256, bias=True)
        (linear_v): Linear(in_features=256, out_features=256, bias=True)
        (linear_p): Sequential(
          (0): Conv1d(3, 3, kernel_size=(1,), stride=(1,))
          (1): BatchNorm1d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv1d(3, 256, kernel_size=(1,), stride=(1,))
        )
        (linear_w): Sequential(
          (0): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1): ReLU(inplace=True)
          (2): Conv1d(256, 32, kernel_size=(1,), stride=(1,))
          (3): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (4): ReLU(inplace=True)
          (5): Conv1d(32, 32, kernel_size=(1,), stride=(1,))
        )
        (softmax): Softmax(dim=1)
      )
      (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (linear3): Linear(in_features=256, out_features=256, bias=False)
      (bn3): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): PointTransformerBlock(
      (linear1): Linear(in_features=256, out_features=256, bias=False)
      (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (transformer2): PointTransformerLayer(
        (linear_q): Linear(in_features=256, out_features=256, bias=True)
        (linear_k): Linear(in_features=256, out_features=256, bias=True)
        (linear_v): Linear(in_features=256, out_features=256, bias=True)
        (linear_p): Sequential(
          (0): Conv1d(3, 3, kernel_size=(1,), stride=(1,))
          (1): BatchNorm1d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv1d(3, 256, kernel_size=(1,), stride=(1,))
        )
        (linear_w): Sequential(
          (0): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1): ReLU(inplace=True)
          (2): Conv1d(256, 32, kernel_size=(1,), stride=(1,))
          (3): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (4): ReLU(inplace=True)
          (5): Conv1d(32, 32, kernel_size=(1,), stride=(1,))
        )
        (softmax): Softmax(dim=1)
      )
      (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (linear3): Linear(in_features=256, out_features=256, bias=False)
      (bn3): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (3): PointTransformerBlock(
      (linear1): Linear(in_features=256, out_features=256, bias=False)
      (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (transformer2): PointTransformerLayer(
        (linear_q): Linear(in_features=256, out_features=256, bias=True)
        (linear_k): Linear(in_features=256, out_features=256, bias=True)
        (linear_v): Linear(in_features=256, out_features=256, bias=True)
        (linear_p): Sequential(
          (0): Conv1d(3, 3, kernel_size=(1,), stride=(1,))
          (1): BatchNorm1d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv1d(3, 256, kernel_size=(1,), stride=(1,))
        )
        (linear_w): Sequential(
          (0): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1): ReLU(inplace=True)
          (2): Conv1d(256, 32, kernel_size=(1,), stride=(1,))
          (3): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (4): ReLU(inplace=True)
          (5): Conv1d(32, 32, kernel_size=(1,), stride=(1,))
        )
        (softmax): Softmax(dim=1)
      )
      (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (linear3): Linear(in_features=256, out_features=256, bias=False)
      (bn3): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (4): PointTransformerBlock(
      (linear1): Linear(in_features=256, out_features=256, bias=False)
      (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (transformer2): PointTransformerLayer(
        (linear_q): Linear(in_features=256, out_features=256, bias=True)
        (linear_k): Linear(in_features=256, out_features=256, bias=True)
        (linear_v): Linear(in_features=256, out_features=256, bias=True)
        (linear_p): Sequential(
          (0): Conv1d(3, 3, kernel_size=(1,), stride=(1,))
          (1): BatchNorm1d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv1d(3, 256, kernel_size=(1,), stride=(1,))
        )
        (linear_w): Sequential(
          (0): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1): ReLU(inplace=True)
          (2): Conv1d(256, 32, kernel_size=(1,), stride=(1,))
          (3): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (4): ReLU(inplace=True)
          (5): Conv1d(32, 32, kernel_size=(1,), stride=(1,))
        )
        (softmax): Softmax(dim=1)
      )
      (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (linear3): Linear(in_features=256, out_features=256, bias=False)
      (bn3): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (5): PointTransformerBlock(
      (linear1): Linear(in_features=256, out_features=256, bias=False)
      (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (transformer2): PointTransformerLayer(
        (linear_q): Linear(in_features=256, out_features=256, bias=True)
        (linear_k): Linear(in_features=256, out_features=256, bias=True)
        (linear_v): Linear(in_features=256, out_features=256, bias=True)
        (linear_p): Sequential(
          (0): Conv1d(3, 3, kernel_size=(1,), stride=(1,))
          (1): BatchNorm1d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv1d(3, 256, kernel_size=(1,), stride=(1,))
        )
        (linear_w): Sequential(
          (0): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1): ReLU(inplace=True)
          (2): Conv1d(256, 32, kernel_size=(1,), stride=(1,))
          (3): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (4): ReLU(inplace=True)
          (5): Conv1d(32, 32, kernel_size=(1,), stride=(1,))
        )
        (softmax): Softmax(dim=1)
      )
      (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (linear3): Linear(in_features=256, out_features=256, bias=False)
      (bn3): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (enc5): Sequential(
    (0): TransitionDown(
      (linear): Linear(in_features=259, out_features=512, bias=False)
      (pool): MaxPool1d(kernel_size=16, stride=16, padding=0, dilation=1, ceil_mode=False)
      (bn): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (1): PointTransformerBlock(
      (linear1): Linear(in_features=512, out_features=512, bias=False)
      (bn1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (transformer2): PointTransformerLayer(
        (linear_q): Linear(in_features=512, out_features=512, bias=True)
        (linear_k): Linear(in_features=512, out_features=512, bias=True)
        (linear_v): Linear(in_features=512, out_features=512, bias=True)
        (linear_p): Sequential(
          (0): Conv1d(3, 3, kernel_size=(1,), stride=(1,))
          (1): BatchNorm1d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv1d(3, 512, kernel_size=(1,), stride=(1,))
        )
        (linear_w): Sequential(
          (0): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1): ReLU(inplace=True)
          (2): Conv1d(512, 64, kernel_size=(1,), stride=(1,))
          (3): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (4): ReLU(inplace=True)
          (5): Conv1d(64, 64, kernel_size=(1,), stride=(1,))
        )
        (softmax): Softmax(dim=1)
      )
      (bn2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (linear3): Linear(in_features=512, out_features=512, bias=False)
      (bn3): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): PointTransformerBlock(
      (linear1): Linear(in_features=512, out_features=512, bias=False)
      (bn1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (transformer2): PointTransformerLayer(
        (linear_q): Linear(in_features=512, out_features=512, bias=True)
        (linear_k): Linear(in_features=512, out_features=512, bias=True)
        (linear_v): Linear(in_features=512, out_features=512, bias=True)
        (linear_p): Sequential(
          (0): Conv1d(3, 3, kernel_size=(1,), stride=(1,))
          (1): BatchNorm1d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv1d(3, 512, kernel_size=(1,), stride=(1,))
        )
        (linear_w): Sequential(
          (0): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1): ReLU(inplace=True)
          (2): Conv1d(512, 64, kernel_size=(1,), stride=(1,))
          (3): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (4): ReLU(inplace=True)
          (5): Conv1d(64, 64, kernel_size=(1,), stride=(1,))
        )
        (softmax): Softmax(dim=1)
      )
      (bn2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (linear3): Linear(in_features=512, out_features=512, bias=False)
      (bn3): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (dec5): Sequential(
    (0): TransitionUp(
      (linear1): Sequential(
        (0): Linear(in_features=1024, out_features=512, bias=True)
        (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
      (linear2): Sequential(
        (0): Linear(in_features=512, out_features=512, bias=True)
        (1): ReLU(inplace=True)
      )
    )
    (1): PointTransformerBlock(
      (linear1): Linear(in_features=512, out_features=512, bias=False)
      (bn1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (transformer2): PointTransformerLayer(
        (linear_q): Linear(in_features=512, out_features=512, bias=True)
        (linear_k): Linear(in_features=512, out_features=512, bias=True)
        (linear_v): Linear(in_features=512, out_features=512, bias=True)
        (linear_p): Sequential(
          (0): Conv1d(3, 3, kernel_size=(1,), stride=(1,))
          (1): BatchNorm1d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv1d(3, 512, kernel_size=(1,), stride=(1,))
        )
        (linear_w): Sequential(
          (0): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1): ReLU(inplace=True)
          (2): Conv1d(512, 64, kernel_size=(1,), stride=(1,))
          (3): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (4): ReLU(inplace=True)
          (5): Conv1d(64, 64, kernel_size=(1,), stride=(1,))
        )
        (softmax): Softmax(dim=1)
      )
      (bn2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (linear3): Linear(in_features=512, out_features=512, bias=False)
      (bn3): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (rt5): RetroTransformer(
    (mlp_bottleneck): Sequential(
      (0): Linear(in_features=512, out_features=32, bias=False)
      (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Linear(in_features=32, out_features=32, bias=False)
    )
    (gav): CrossAttention(
      (linear_q): Linear(in_features=32, out_features=32, bias=True)
      (linear_k): Linear(in_features=32, out_features=32, bias=True)
      (linear_v): Linear(in_features=32, out_features=32, bias=True)
      (linear_p): Sequential(
        (0): Conv1d(3, 3, kernel_size=(1,), stride=(1,))
        (1): BatchNorm1d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
        (3): Conv1d(3, 2, kernel_size=(1,), stride=(1,))
      )
      (softmax): Softmax(dim=1)
    )
    (mlp_out): Sequential(
      (0): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): ReLU(inplace=True)
      (2): Linear(in_features=32, out_features=32, bias=False)
      (3): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (linear_info_curr): Linear(in_features=512, out_features=32, bias=True)
    (mlp_gate): Sequential(
      (0): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): Linear(in_features=32, out_features=32, bias=False)
      (2): Sigmoid()
    )
  )
  (cls5): Sequential(
    (0): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (1): ReLU()
    (2): Linear(in_features=32, out_features=13, bias=True)
  )
  (dec4): Sequential(
    (0): TransitionUp(
      (linear1): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
      (linear2): Sequential(
        (0): Linear(in_features=512, out_features=256, bias=True)
        (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
    )
    (1): PointTransformerBlock(
      (linear1): Linear(in_features=256, out_features=256, bias=False)
      (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (transformer2): PointTransformerLayer(
        (linear_q): Linear(in_features=256, out_features=256, bias=True)
        (linear_k): Linear(in_features=256, out_features=256, bias=True)
        (linear_v): Linear(in_features=256, out_features=256, bias=True)
        (linear_p): Sequential(
          (0): Conv1d(3, 3, kernel_size=(1,), stride=(1,))
          (1): BatchNorm1d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv1d(3, 256, kernel_size=(1,), stride=(1,))
        )
        (linear_w): Sequential(
          (0): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1): ReLU(inplace=True)
          (2): Conv1d(256, 32, kernel_size=(1,), stride=(1,))
          (3): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (4): ReLU(inplace=True)
          (5): Conv1d(32, 32, kernel_size=(1,), stride=(1,))
        )
        (softmax): Softmax(dim=1)
      )
      (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (linear3): Linear(in_features=256, out_features=256, bias=False)
      (bn3): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (rt4): RetroTransformer(
    (mlp_bottleneck): Sequential(
      (0): Linear(in_features=256, out_features=32, bias=False)
      (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Linear(in_features=32, out_features=32, bias=False)
    )
    (gav): CrossAttention(
      (linear_q): Linear(in_features=32, out_features=32, bias=True)
      (linear_k): Linear(in_features=32, out_features=32, bias=True)
      (linear_v): Linear(in_features=32, out_features=32, bias=True)
      (linear_p): Sequential(
        (0): Conv1d(3, 3, kernel_size=(1,), stride=(1,))
        (1): BatchNorm1d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
        (3): Conv1d(3, 2, kernel_size=(1,), stride=(1,))
      )
      (softmax): Softmax(dim=1)
    )
    (mlp_out): Sequential(
      (0): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): ReLU(inplace=True)
      (2): Linear(in_features=32, out_features=32, bias=False)
      (3): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (linear_info_curr): Linear(in_features=256, out_features=32, bias=True)
    (mlp_gate): Sequential(
      (0): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): Linear(in_features=32, out_features=32, bias=False)
      (2): Sigmoid()
    )
  )
  (cls4): Sequential(
    (0): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (1): ReLU()
    (2): Linear(in_features=32, out_features=13, bias=True)
  )
  (dec3): Sequential(
    (0): TransitionUp(
      (linear1): Sequential(
        (0): Linear(in_features=128, out_features=128, bias=True)
        (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
      (linear2): Sequential(
        (0): Linear(in_features=256, out_features=128, bias=True)
        (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
    )
    (1): PointTransformerBlock(
      (linear1): Linear(in_features=128, out_features=128, bias=False)
      (bn1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (transformer2): PointTransformerLayer(
        (linear_q): Linear(in_features=128, out_features=128, bias=True)
        (linear_k): Linear(in_features=128, out_features=128, bias=True)
        (linear_v): Linear(in_features=128, out_features=128, bias=True)
        (linear_p): Sequential(
          (0): Conv1d(3, 3, kernel_size=(1,), stride=(1,))
          (1): BatchNorm1d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv1d(3, 128, kernel_size=(1,), stride=(1,))
        )
        (linear_w): Sequential(
          (0): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1): ReLU(inplace=True)
          (2): Conv1d(128, 16, kernel_size=(1,), stride=(1,))
          (3): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (4): ReLU(inplace=True)
          (5): Conv1d(16, 16, kernel_size=(1,), stride=(1,))
        )
        (softmax): Softmax(dim=1)
      )
      (bn2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (linear3): Linear(in_features=128, out_features=128, bias=False)
      (bn3): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (rt3): RetroTransformer(
    (mlp_bottleneck): Sequential(
      (0): Linear(in_features=128, out_features=32, bias=False)
      (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Linear(in_features=32, out_features=32, bias=False)
    )
    (gav): CrossAttention(
      (linear_q): Linear(in_features=32, out_features=32, bias=True)
      (linear_k): Linear(in_features=32, out_features=32, bias=True)
      (linear_v): Linear(in_features=32, out_features=32, bias=True)
      (linear_p): Sequential(
        (0): Conv1d(3, 3, kernel_size=(1,), stride=(1,))
        (1): BatchNorm1d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
        (3): Conv1d(3, 2, kernel_size=(1,), stride=(1,))
      )
      (softmax): Softmax(dim=1)
    )
    (mlp_out): Sequential(
      (0): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): ReLU(inplace=True)
      (2): Linear(in_features=32, out_features=32, bias=False)
      (3): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (linear_info_curr): Linear(in_features=128, out_features=32, bias=True)
    (mlp_gate): Sequential(
      (0): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): Linear(in_features=32, out_features=32, bias=False)
      (2): Sigmoid()
    )
  )
  (cls3): Sequential(
    (0): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (1): ReLU()
    (2): Linear(in_features=32, out_features=13, bias=True)
  )
  (dec2): Sequential(
    (0): TransitionUp(
      (linear1): Sequential(
        (0): Linear(in_features=64, out_features=64, bias=True)
        (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
      (linear2): Sequential(
        (0): Linear(in_features=128, out_features=64, bias=True)
        (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
    )
    (1): PointTransformerBlock(
      (linear1): Linear(in_features=64, out_features=64, bias=False)
      (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (transformer2): PointTransformerLayer(
        (linear_q): Linear(in_features=64, out_features=64, bias=True)
        (linear_k): Linear(in_features=64, out_features=64, bias=True)
        (linear_v): Linear(in_features=64, out_features=64, bias=True)
        (linear_p): Sequential(
          (0): Conv1d(3, 3, kernel_size=(1,), stride=(1,))
          (1): BatchNorm1d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv1d(3, 64, kernel_size=(1,), stride=(1,))
        )
        (linear_w): Sequential(
          (0): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1): ReLU(inplace=True)
          (2): Conv1d(64, 8, kernel_size=(1,), stride=(1,))
          (3): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (4): ReLU(inplace=True)
          (5): Conv1d(8, 8, kernel_size=(1,), stride=(1,))
        )
        (softmax): Softmax(dim=1)
      )
      (bn2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (linear3): Linear(in_features=64, out_features=64, bias=False)
      (bn3): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (rt2): RetroTransformer(
    (mlp_bottleneck): Sequential(
      (0): Linear(in_features=64, out_features=32, bias=False)
      (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Linear(in_features=32, out_features=32, bias=False)
    )
    (gav): CrossAttention(
      (linear_q): Linear(in_features=32, out_features=32, bias=True)
      (linear_k): Linear(in_features=32, out_features=32, bias=True)
      (linear_v): Linear(in_features=32, out_features=32, bias=True)
      (linear_p): Sequential(
        (0): Conv1d(3, 3, kernel_size=(1,), stride=(1,))
        (1): BatchNorm1d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
        (3): Conv1d(3, 2, kernel_size=(1,), stride=(1,))
      )
      (softmax): Softmax(dim=1)
    )
    (mlp_out): Sequential(
      (0): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): ReLU(inplace=True)
      (2): Linear(in_features=32, out_features=32, bias=False)
      (3): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (linear_info_curr): Linear(in_features=64, out_features=32, bias=True)
    (mlp_gate): Sequential(
      (0): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): Linear(in_features=32, out_features=32, bias=False)
      (2): Sigmoid()
    )
  )
  (cls2): Sequential(
    (0): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (1): ReLU()
    (2): Linear(in_features=32, out_features=13, bias=True)
  )
  (dec1): Sequential(
    (0): TransitionUp(
      (linear1): Sequential(
        (0): Linear(in_features=32, out_features=32, bias=True)
        (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
      (linear2): Sequential(
        (0): Linear(in_features=64, out_features=32, bias=True)
        (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
    )
    (1): PointTransformerBlock(
      (linear1): Linear(in_features=32, out_features=32, bias=False)
      (bn1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (transformer2): PointTransformerLayer(
        (linear_q): Linear(in_features=32, out_features=32, bias=True)
        (linear_k): Linear(in_features=32, out_features=32, bias=True)
        (linear_v): Linear(in_features=32, out_features=32, bias=True)
        (linear_p): Sequential(
          (0): Conv1d(3, 3, kernel_size=(1,), stride=(1,))
          (1): BatchNorm1d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv1d(3, 32, kernel_size=(1,), stride=(1,))
        )
        (linear_w): Sequential(
          (0): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1): ReLU(inplace=True)
          (2): Conv1d(32, 4, kernel_size=(1,), stride=(1,))
          (3): BatchNorm1d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (4): ReLU(inplace=True)
          (5): Conv1d(4, 4, kernel_size=(1,), stride=(1,))
        )
        (softmax): Softmax(dim=1)
      )
      (bn2): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (linear3): Linear(in_features=32, out_features=32, bias=False)
      (bn3): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (rt1): RetroTransformer(
    (mlp_bottleneck): Sequential(
      (0): Linear(in_features=32, out_features=32, bias=False)
      (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Linear(in_features=32, out_features=32, bias=False)
    )
    (gav): CrossAttention(
      (linear_q): Linear(in_features=32, out_features=32, bias=True)
      (linear_k): Linear(in_features=32, out_features=32, bias=True)
      (linear_v): Linear(in_features=32, out_features=32, bias=True)
      (linear_p): Sequential(
        (0): Conv1d(3, 3, kernel_size=(1,), stride=(1,))
        (1): BatchNorm1d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
        (3): Conv1d(3, 2, kernel_size=(1,), stride=(1,))
      )
      (softmax): Softmax(dim=1)
    )
    (mlp_out): Sequential(
      (0): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): ReLU(inplace=True)
      (2): Linear(in_features=32, out_features=32, bias=False)
      (3): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (linear_info_curr): Linear(in_features=32, out_features=32, bias=True)
    (mlp_gate): Sequential(
      (0): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): Linear(in_features=32, out_features=32, bias=False)
      (2): Sigmoid()
    )
  )
  (cls1): Sequential(
    (0): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (1): ReLU()
    (2): Linear(in_features=32, out_features=13, bias=True)
  )
)
[2023-11-04 15:56:03,720 INFO test.py line 72 3258548] => loading checkpoint './checkpoints/model_best.pth'
[2023-11-04 15:56:05,033 INFO test.py line 80 3258548] => loaded checkpoint './checkpoints/model_best.pth' (epoch 66)
[2023-11-04 15:56:05,034 INFO test.py line 124 3258548] >>>>>>>>>>>>>>>> Start Evaluation >>>>>>>>>>>>>>>>
[2023-11-04 15:56:10,276 INFO test.py line 148 3258548] 1/68: 1/65/47358, XPArea_5_WC_1
[2023-11-04 15:56:10,287 INFO test.py line 148 3258548] 1/68: 2/65/47358, XPArea_5_WC_1
[2023-11-04 15:56:10,297 INFO test.py line 148 3258548] 1/68: 3/65/47358, XPArea_5_WC_1
[2023-11-04 15:56:10,308 INFO test.py line 148 3258548] 1/68: 4/65/47358, XPArea_5_WC_1
[2023-11-04 15:56:10,319 INFO test.py line 148 3258548] 1/68: 5/65/47358, XPArea_5_WC_1
[2023-11-04 15:56:10,333 INFO test.py line 148 3258548] 1/68: 6/65/47358, XPArea_5_WC_1
[2023-11-04 15:56:10,346 INFO test.py line 148 3258548] 1/68: 7/65/47358, XPArea_5_WC_1
[2023-11-04 15:56:10,359 INFO test.py line 148 3258548] 1/68: 8/65/47358, XPArea_5_WC_1
[2023-11-04 15:56:10,371 INFO test.py line 148 3258548] 1/68: 9/65/47358, XPArea_5_WC_1
[2023-11-04 15:56:10,382 INFO test.py line 148 3258548] 1/68: 10/65/47358, XPArea_5_WC_1
[2023-11-04 15:56:10,394 INFO test.py line 148 3258548] 1/68: 11/65/47358, XPArea_5_WC_1
[2023-11-04 15:56:10,405 INFO test.py line 148 3258548] 1/68: 12/65/47358, XPArea_5_WC_1
[2023-11-04 15:56:10,416 INFO test.py line 148 3258548] 1/68: 13/65/47358, XPArea_5_WC_1
[2023-11-04 15:56:10,427 INFO test.py line 148 3258548] 1/68: 14/65/47358, XPArea_5_WC_1
[2023-11-04 15:56:10,438 INFO test.py line 148 3258548] 1/68: 15/65/47358, XPArea_5_WC_1
[2023-11-04 15:56:10,449 INFO test.py line 148 3258548] 1/68: 16/65/47358, XPArea_5_WC_1
[2023-11-04 15:56:10,461 INFO test.py line 148 3258548] 1/68: 17/65/47358, XPArea_5_WC_1
[2023-11-04 15:56:10,474 INFO test.py line 148 3258548] 1/68: 18/65/47358, XPArea_5_WC_1
[2023-11-04 15:56:10,486 INFO test.py line 148 3258548] 1/68: 19/65/47358, XPArea_5_WC_1
[2023-11-04 15:56:10,499 INFO test.py line 148 3258548] 1/68: 20/65/47358, XPArea_5_WC_1
[2023-11-04 15:56:10,512 INFO test.py line 148 3258548] 1/68: 21/65/47358, XPArea_5_WC_1
[2023-11-04 15:56:10,527 INFO test.py line 148 3258548] 1/68: 22/65/47358, XPArea_5_WC_1
[2023-11-04 15:56:10,538 INFO test.py line 148 3258548] 1/68: 23/65/47358, XPArea_5_WC_1
[2023-11-04 15:56:10,549 INFO test.py line 148 3258548] 1/68: 24/65/47358, XPArea_5_WC_1
[2023-11-04 15:56:10,560 INFO test.py line 148 3258548] 1/68: 25/65/47358, XPArea_5_WC_1
[2023-11-04 15:56:10,571 INFO test.py line 148 3258548] 1/68: 26/65/47358, XPArea_5_WC_1
[2023-11-04 15:56:10,582 INFO test.py line 148 3258548] 1/68: 27/65/47358, XPArea_5_WC_1
[2023-11-04 15:56:10,594 INFO test.py line 148 3258548] 1/68: 28/65/47358, XPArea_5_WC_1
[2023-11-04 15:56:10,606 INFO test.py line 148 3258548] 1/68: 29/65/47358, XPArea_5_WC_1
[2023-11-04 15:56:10,617 INFO test.py line 148 3258548] 1/68: 30/65/47358, XPArea_5_WC_1
[2023-11-04 15:56:10,629 INFO test.py line 148 3258548] 1/68: 31/65/47358, XPArea_5_WC_1
[2023-11-04 15:56:10,640 INFO test.py line 148 3258548] 1/68: 32/65/47358, XPArea_5_WC_1
[2023-11-04 15:56:10,651 INFO test.py line 148 3258548] 1/68: 33/65/47358, XPArea_5_WC_1
[2023-11-04 15:56:10,663 INFO test.py line 148 3258548] 1/68: 34/65/47358, XPArea_5_WC_1
[2023-11-04 15:56:10,675 INFO test.py line 148 3258548] 1/68: 35/65/47358, XPArea_5_WC_1
[2023-11-04 15:56:10,688 INFO test.py line 148 3258548] 1/68: 36/65/47358, XPArea_5_WC_1
[2023-11-04 15:56:10,699 INFO test.py line 148 3258548] 1/68: 37/65/47358, XPArea_5_WC_1
[2023-11-04 15:56:10,710 INFO test.py line 148 3258548] 1/68: 38/65/47358, XPArea_5_WC_1
[2023-11-04 15:56:10,722 INFO test.py line 148 3258548] 1/68: 39/65/47358, XPArea_5_WC_1
[2023-11-04 15:56:10,739 INFO test.py line 148 3258548] 1/68: 40/65/47358, XPArea_5_WC_1
[2023-11-04 15:56:10,751 INFO test.py line 148 3258548] 1/68: 41/65/47358, XPArea_5_WC_1
[2023-11-04 15:56:10,762 INFO test.py line 148 3258548] 1/68: 42/65/47358, XPArea_5_WC_1
[2023-11-04 15:56:10,773 INFO test.py line 148 3258548] 1/68: 43/65/47358, XPArea_5_WC_1
[2023-11-04 15:56:10,784 INFO test.py line 148 3258548] 1/68: 44/65/47358, XPArea_5_WC_1
[2023-11-04 15:56:10,794 INFO test.py line 148 3258548] 1/68: 45/65/47358, XPArea_5_WC_1
[2023-11-04 15:56:10,804 INFO test.py line 148 3258548] 1/68: 46/65/47358, XPArea_5_WC_1
[2023-11-04 15:56:10,814 INFO test.py line 148 3258548] 1/68: 47/65/47358, XPArea_5_WC_1
[2023-11-04 15:56:10,830 INFO test.py line 148 3258548] 1/68: 48/65/47358, XPArea_5_WC_1
[2023-11-04 15:56:10,842 INFO test.py line 148 3258548] 1/68: 49/65/47358, XPArea_5_WC_1
[2023-11-04 15:56:10,855 INFO test.py line 148 3258548] 1/68: 50/65/47358, XPArea_5_WC_1
[2023-11-04 15:56:10,867 INFO test.py line 148 3258548] 1/68: 51/65/47358, XPArea_5_WC_1
[2023-11-04 15:56:10,879 INFO test.py line 148 3258548] 1/68: 52/65/47358, XPArea_5_WC_1
[2023-11-04 15:56:10,892 INFO test.py line 148 3258548] 1/68: 53/65/47358, XPArea_5_WC_1
[2023-11-04 15:56:10,904 INFO test.py line 148 3258548] 1/68: 54/65/47358, XPArea_5_WC_1
[2023-11-04 15:56:10,917 INFO test.py line 148 3258548] 1/68: 55/65/47358, XPArea_5_WC_1
[2023-11-04 15:56:10,930 INFO test.py line 148 3258548] 1/68: 56/65/47358, XPArea_5_WC_1
[2023-11-04 15:56:10,942 INFO test.py line 148 3258548] 1/68: 57/65/47358, XPArea_5_WC_1
[2023-11-04 15:56:10,954 INFO test.py line 148 3258548] 1/68: 58/65/47358, XPArea_5_WC_1
[2023-11-04 15:56:10,966 INFO test.py line 148 3258548] 1/68: 59/65/47358, XPArea_5_WC_1
[2023-11-04 15:56:10,981 INFO test.py line 148 3258548] 1/68: 60/65/47358, XPArea_5_WC_1
[2023-11-04 15:56:10,993 INFO test.py line 148 3258548] 1/68: 61/65/47358, XPArea_5_WC_1
[2023-11-04 15:56:11,006 INFO test.py line 148 3258548] 1/68: 62/65/47358, XPArea_5_WC_1
[2023-11-04 15:56:11,018 INFO test.py line 148 3258548] 1/68: 63/65/47358, XPArea_5_WC_1
[2023-11-04 15:56:11,030 INFO test.py line 148 3258548] 1/68: 64/65/47358, XPArea_5_WC_1
[2023-11-04 15:56:11,041 INFO test.py line 148 3258548] 1/68: 65/65/47358, XPArea_5_WC_1
[2023-11-05 13:25:30,561 INFO test.py line 56 3951937] arch: retro_fpn
base_lr: 0.1
batch_size: 4
batch_size_test: 4
batch_size_val: 4
classes: 13
data_name: s3dis
data_root: /data1/xp/data/s3dis/trainval_fullarea
dist_backend: nccl
dist_url: tcp://localhost:8888
drop_rate: 0.5
epochs: 100
eval_freq: 1
evaluate: True
fea_dim: 6
ignore_label: 255
loop: 10
manual_seed: 7777
model_path: ./checkpoints/model_best.pth
momentum: 0.9
multiplier: 0.1
multiprocessing_distributed: True
n_samples_retro: 16
names_path: data/s3dis/s3dis_names.txt
print_freq: 1
rank: 0
resume: None
save_folder: exp
save_freq: 1
save_path: exp/retro_fpn
split: val
start_epoch: 0
step_epoch: 30
sync_bn: False
test_area: 5
test_gpu: [7]
test_list: dataset/s3dis/list/val5.txt
test_list_full: dataset/s3dis/list/val5_full.txt
test_workers: 4
train_gpu: [7]
use_xyz: True
voxel_max: 80000
voxel_size: 0.04
weight: None
weight_decay: 0.0001
workers: 4
world_size: 1
[2023-11-05 13:25:31,245 INFO test.py line 58 3951937] => creating model ...
[2023-11-05 13:25:31,245 INFO test.py line 59 3951937] Classes: 13
[2023-11-05 13:26:55,342 INFO test.py line 56 3961149] arch: retro_fpn
base_lr: 0.1
batch_size: 4
batch_size_test: 4
batch_size_val: 4
classes: 13
data_name: s3dis
data_root: /data1/xp/data/s3dis/trainval_fullarea
dist_backend: nccl
dist_url: tcp://localhost:8888
drop_rate: 0.5
epochs: 100
eval_freq: 1
evaluate: True
fea_dim: 6
ignore_label: 255
loop: 10
manual_seed: 7777
model_path: ./checkpoints/model_best.pth
momentum: 0.9
multiplier: 0.1
multiprocessing_distributed: True
n_samples_retro: 16
names_path: data/s3dis/s3dis_names.txt
print_freq: 1
rank: 0
resume: None
save_folder: exp
save_freq: 1
save_path: exp/retro_fpn
split: val
start_epoch: 0
step_epoch: 30
sync_bn: False
test_area: 5
test_gpu: [7]
test_list: dataset/s3dis/list/val5.txt
test_list_full: dataset/s3dis/list/val5_full.txt
test_workers: 4
train_gpu: [7]
use_xyz: True
voxel_max: 80000
voxel_size: 0.04
weight: None
weight_decay: 0.0001
workers: 4
world_size: 1
[2023-11-05 13:26:55,343 INFO test.py line 58 3961149] => creating model ...
[2023-11-05 13:26:55,343 INFO test.py line 59 3961149] Classes: 13
[2023-11-05 13:28:32,402 INFO test.py line 57 3968969] arch: retro_fpn
base_lr: 0.1
batch_size: 4
batch_size_test: 4
batch_size_val: 4
classes: 13
data_name: s3dis
data_root: /data1/xp/data/s3dis/trainval_fullarea
dist_backend: nccl
dist_url: tcp://localhost:8888
drop_rate: 0.5
epochs: 100
eval_freq: 1
evaluate: True
fea_dim: 6
ignore_label: 255
loop: 10
manual_seed: 7777
model_path: ./checkpoints/model_best.pth
momentum: 0.9
multiplier: 0.1
multiprocessing_distributed: True
n_samples_retro: 16
names_path: data/s3dis/s3dis_names.txt
print_freq: 1
rank: 0
resume: None
save_folder: exp
save_freq: 1
save_path: exp/retro_fpn
split: val
start_epoch: 0
step_epoch: 30
sync_bn: False
test_area: 5
test_gpu: [7]
test_list: dataset/s3dis/list/val5.txt
test_list_full: dataset/s3dis/list/val5_full.txt
test_workers: 4
train_gpu: [7]
use_xyz: True
voxel_max: 80000
voxel_size: 0.04
weight: None
weight_decay: 0.0001
workers: 4
world_size: 1
[2023-11-05 13:28:32,402 INFO test.py line 59 3968969] => creating model ...
[2023-11-05 13:28:32,402 INFO test.py line 60 3968969] Classes: 13
[2023-11-05 13:28:38,913 INFO test.py line 69 3968969] RetroFPNSeg(
  (enc1): Sequential(
    (0): TransitionDown(
      (linear): Linear(in_features=6, out_features=32, bias=False)
      (bn): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (1): PointTransformerBlock(
      (linear1): Linear(in_features=32, out_features=32, bias=False)
      (bn1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (transformer2): PointTransformerLayer(
        (linear_q): Linear(in_features=32, out_features=32, bias=True)
        (linear_k): Linear(in_features=32, out_features=32, bias=True)
        (linear_v): Linear(in_features=32, out_features=32, bias=True)
        (linear_p): Sequential(
          (0): Conv1d(3, 3, kernel_size=(1,), stride=(1,))
          (1): BatchNorm1d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv1d(3, 32, kernel_size=(1,), stride=(1,))
        )
        (linear_w): Sequential(
          (0): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1): ReLU(inplace=True)
          (2): Conv1d(32, 4, kernel_size=(1,), stride=(1,))
          (3): BatchNorm1d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (4): ReLU(inplace=True)
          (5): Conv1d(4, 4, kernel_size=(1,), stride=(1,))
        )
        (softmax): Softmax(dim=1)
      )
      (bn2): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (linear3): Linear(in_features=32, out_features=32, bias=False)
      (bn3): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (enc2): Sequential(
    (0): TransitionDown(
      (linear): Linear(in_features=35, out_features=64, bias=False)
      (pool): MaxPool1d(kernel_size=16, stride=16, padding=0, dilation=1, ceil_mode=False)
      (bn): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (1): PointTransformerBlock(
      (linear1): Linear(in_features=64, out_features=64, bias=False)
      (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (transformer2): PointTransformerLayer(
        (linear_q): Linear(in_features=64, out_features=64, bias=True)
        (linear_k): Linear(in_features=64, out_features=64, bias=True)
        (linear_v): Linear(in_features=64, out_features=64, bias=True)
        (linear_p): Sequential(
          (0): Conv1d(3, 3, kernel_size=(1,), stride=(1,))
          (1): BatchNorm1d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv1d(3, 64, kernel_size=(1,), stride=(1,))
        )
        (linear_w): Sequential(
          (0): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1): ReLU(inplace=True)
          (2): Conv1d(64, 8, kernel_size=(1,), stride=(1,))
          (3): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (4): ReLU(inplace=True)
          (5): Conv1d(8, 8, kernel_size=(1,), stride=(1,))
        )
        (softmax): Softmax(dim=1)
      )
      (bn2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (linear3): Linear(in_features=64, out_features=64, bias=False)
      (bn3): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): PointTransformerBlock(
      (linear1): Linear(in_features=64, out_features=64, bias=False)
      (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (transformer2): PointTransformerLayer(
        (linear_q): Linear(in_features=64, out_features=64, bias=True)
        (linear_k): Linear(in_features=64, out_features=64, bias=True)
        (linear_v): Linear(in_features=64, out_features=64, bias=True)
        (linear_p): Sequential(
          (0): Conv1d(3, 3, kernel_size=(1,), stride=(1,))
          (1): BatchNorm1d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv1d(3, 64, kernel_size=(1,), stride=(1,))
        )
        (linear_w): Sequential(
          (0): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1): ReLU(inplace=True)
          (2): Conv1d(64, 8, kernel_size=(1,), stride=(1,))
          (3): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (4): ReLU(inplace=True)
          (5): Conv1d(8, 8, kernel_size=(1,), stride=(1,))
        )
        (softmax): Softmax(dim=1)
      )
      (bn2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (linear3): Linear(in_features=64, out_features=64, bias=False)
      (bn3): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (enc3): Sequential(
    (0): TransitionDown(
      (linear): Linear(in_features=67, out_features=128, bias=False)
      (pool): MaxPool1d(kernel_size=16, stride=16, padding=0, dilation=1, ceil_mode=False)
      (bn): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (1): PointTransformerBlock(
      (linear1): Linear(in_features=128, out_features=128, bias=False)
      (bn1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (transformer2): PointTransformerLayer(
        (linear_q): Linear(in_features=128, out_features=128, bias=True)
        (linear_k): Linear(in_features=128, out_features=128, bias=True)
        (linear_v): Linear(in_features=128, out_features=128, bias=True)
        (linear_p): Sequential(
          (0): Conv1d(3, 3, kernel_size=(1,), stride=(1,))
          (1): BatchNorm1d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv1d(3, 128, kernel_size=(1,), stride=(1,))
        )
        (linear_w): Sequential(
          (0): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1): ReLU(inplace=True)
          (2): Conv1d(128, 16, kernel_size=(1,), stride=(1,))
          (3): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (4): ReLU(inplace=True)
          (5): Conv1d(16, 16, kernel_size=(1,), stride=(1,))
        )
        (softmax): Softmax(dim=1)
      )
      (bn2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (linear3): Linear(in_features=128, out_features=128, bias=False)
      (bn3): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): PointTransformerBlock(
      (linear1): Linear(in_features=128, out_features=128, bias=False)
      (bn1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (transformer2): PointTransformerLayer(
        (linear_q): Linear(in_features=128, out_features=128, bias=True)
        (linear_k): Linear(in_features=128, out_features=128, bias=True)
        (linear_v): Linear(in_features=128, out_features=128, bias=True)
        (linear_p): Sequential(
          (0): Conv1d(3, 3, kernel_size=(1,), stride=(1,))
          (1): BatchNorm1d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv1d(3, 128, kernel_size=(1,), stride=(1,))
        )
        (linear_w): Sequential(
          (0): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1): ReLU(inplace=True)
          (2): Conv1d(128, 16, kernel_size=(1,), stride=(1,))
          (3): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (4): ReLU(inplace=True)
          (5): Conv1d(16, 16, kernel_size=(1,), stride=(1,))
        )
        (softmax): Softmax(dim=1)
      )
      (bn2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (linear3): Linear(in_features=128, out_features=128, bias=False)
      (bn3): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (3): PointTransformerBlock(
      (linear1): Linear(in_features=128, out_features=128, bias=False)
      (bn1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (transformer2): PointTransformerLayer(
        (linear_q): Linear(in_features=128, out_features=128, bias=True)
        (linear_k): Linear(in_features=128, out_features=128, bias=True)
        (linear_v): Linear(in_features=128, out_features=128, bias=True)
        (linear_p): Sequential(
          (0): Conv1d(3, 3, kernel_size=(1,), stride=(1,))
          (1): BatchNorm1d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv1d(3, 128, kernel_size=(1,), stride=(1,))
        )
        (linear_w): Sequential(
          (0): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1): ReLU(inplace=True)
          (2): Conv1d(128, 16, kernel_size=(1,), stride=(1,))
          (3): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (4): ReLU(inplace=True)
          (5): Conv1d(16, 16, kernel_size=(1,), stride=(1,))
        )
        (softmax): Softmax(dim=1)
      )
      (bn2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (linear3): Linear(in_features=128, out_features=128, bias=False)
      (bn3): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (enc4): Sequential(
    (0): TransitionDown(
      (linear): Linear(in_features=131, out_features=256, bias=False)
      (pool): MaxPool1d(kernel_size=16, stride=16, padding=0, dilation=1, ceil_mode=False)
      (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (1): PointTransformerBlock(
      (linear1): Linear(in_features=256, out_features=256, bias=False)
      (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (transformer2): PointTransformerLayer(
        (linear_q): Linear(in_features=256, out_features=256, bias=True)
        (linear_k): Linear(in_features=256, out_features=256, bias=True)
        (linear_v): Linear(in_features=256, out_features=256, bias=True)
        (linear_p): Sequential(
          (0): Conv1d(3, 3, kernel_size=(1,), stride=(1,))
          (1): BatchNorm1d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv1d(3, 256, kernel_size=(1,), stride=(1,))
        )
        (linear_w): Sequential(
          (0): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1): ReLU(inplace=True)
          (2): Conv1d(256, 32, kernel_size=(1,), stride=(1,))
          (3): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (4): ReLU(inplace=True)
          (5): Conv1d(32, 32, kernel_size=(1,), stride=(1,))
        )
        (softmax): Softmax(dim=1)
      )
      (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (linear3): Linear(in_features=256, out_features=256, bias=False)
      (bn3): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): PointTransformerBlock(
      (linear1): Linear(in_features=256, out_features=256, bias=False)
      (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (transformer2): PointTransformerLayer(
        (linear_q): Linear(in_features=256, out_features=256, bias=True)
        (linear_k): Linear(in_features=256, out_features=256, bias=True)
        (linear_v): Linear(in_features=256, out_features=256, bias=True)
        (linear_p): Sequential(
          (0): Conv1d(3, 3, kernel_size=(1,), stride=(1,))
          (1): BatchNorm1d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv1d(3, 256, kernel_size=(1,), stride=(1,))
        )
        (linear_w): Sequential(
          (0): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1): ReLU(inplace=True)
          (2): Conv1d(256, 32, kernel_size=(1,), stride=(1,))
          (3): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (4): ReLU(inplace=True)
          (5): Conv1d(32, 32, kernel_size=(1,), stride=(1,))
        )
        (softmax): Softmax(dim=1)
      )
      (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (linear3): Linear(in_features=256, out_features=256, bias=False)
      (bn3): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (3): PointTransformerBlock(
      (linear1): Linear(in_features=256, out_features=256, bias=False)
      (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (transformer2): PointTransformerLayer(
        (linear_q): Linear(in_features=256, out_features=256, bias=True)
        (linear_k): Linear(in_features=256, out_features=256, bias=True)
        (linear_v): Linear(in_features=256, out_features=256, bias=True)
        (linear_p): Sequential(
          (0): Conv1d(3, 3, kernel_size=(1,), stride=(1,))
          (1): BatchNorm1d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv1d(3, 256, kernel_size=(1,), stride=(1,))
        )
        (linear_w): Sequential(
          (0): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1): ReLU(inplace=True)
          (2): Conv1d(256, 32, kernel_size=(1,), stride=(1,))
          (3): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (4): ReLU(inplace=True)
          (5): Conv1d(32, 32, kernel_size=(1,), stride=(1,))
        )
        (softmax): Softmax(dim=1)
      )
      (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (linear3): Linear(in_features=256, out_features=256, bias=False)
      (bn3): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (4): PointTransformerBlock(
      (linear1): Linear(in_features=256, out_features=256, bias=False)
      (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (transformer2): PointTransformerLayer(
        (linear_q): Linear(in_features=256, out_features=256, bias=True)
        (linear_k): Linear(in_features=256, out_features=256, bias=True)
        (linear_v): Linear(in_features=256, out_features=256, bias=True)
        (linear_p): Sequential(
          (0): Conv1d(3, 3, kernel_size=(1,), stride=(1,))
          (1): BatchNorm1d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv1d(3, 256, kernel_size=(1,), stride=(1,))
        )
        (linear_w): Sequential(
          (0): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1): ReLU(inplace=True)
          (2): Conv1d(256, 32, kernel_size=(1,), stride=(1,))
          (3): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (4): ReLU(inplace=True)
          (5): Conv1d(32, 32, kernel_size=(1,), stride=(1,))
        )
        (softmax): Softmax(dim=1)
      )
      (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (linear3): Linear(in_features=256, out_features=256, bias=False)
      (bn3): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (5): PointTransformerBlock(
      (linear1): Linear(in_features=256, out_features=256, bias=False)
      (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (transformer2): PointTransformerLayer(
        (linear_q): Linear(in_features=256, out_features=256, bias=True)
        (linear_k): Linear(in_features=256, out_features=256, bias=True)
        (linear_v): Linear(in_features=256, out_features=256, bias=True)
        (linear_p): Sequential(
          (0): Conv1d(3, 3, kernel_size=(1,), stride=(1,))
          (1): BatchNorm1d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv1d(3, 256, kernel_size=(1,), stride=(1,))
        )
        (linear_w): Sequential(
          (0): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1): ReLU(inplace=True)
          (2): Conv1d(256, 32, kernel_size=(1,), stride=(1,))
          (3): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (4): ReLU(inplace=True)
          (5): Conv1d(32, 32, kernel_size=(1,), stride=(1,))
        )
        (softmax): Softmax(dim=1)
      )
      (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (linear3): Linear(in_features=256, out_features=256, bias=False)
      (bn3): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (enc5): Sequential(
    (0): TransitionDown(
      (linear): Linear(in_features=259, out_features=512, bias=False)
      (pool): MaxPool1d(kernel_size=16, stride=16, padding=0, dilation=1, ceil_mode=False)
      (bn): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (1): PointTransformerBlock(
      (linear1): Linear(in_features=512, out_features=512, bias=False)
      (bn1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (transformer2): PointTransformerLayer(
        (linear_q): Linear(in_features=512, out_features=512, bias=True)
        (linear_k): Linear(in_features=512, out_features=512, bias=True)
        (linear_v): Linear(in_features=512, out_features=512, bias=True)
        (linear_p): Sequential(
          (0): Conv1d(3, 3, kernel_size=(1,), stride=(1,))
          (1): BatchNorm1d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv1d(3, 512, kernel_size=(1,), stride=(1,))
        )
        (linear_w): Sequential(
          (0): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1): ReLU(inplace=True)
          (2): Conv1d(512, 64, kernel_size=(1,), stride=(1,))
          (3): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (4): ReLU(inplace=True)
          (5): Conv1d(64, 64, kernel_size=(1,), stride=(1,))
        )
        (softmax): Softmax(dim=1)
      )
      (bn2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (linear3): Linear(in_features=512, out_features=512, bias=False)
      (bn3): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): PointTransformerBlock(
      (linear1): Linear(in_features=512, out_features=512, bias=False)
      (bn1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (transformer2): PointTransformerLayer(
        (linear_q): Linear(in_features=512, out_features=512, bias=True)
        (linear_k): Linear(in_features=512, out_features=512, bias=True)
        (linear_v): Linear(in_features=512, out_features=512, bias=True)
        (linear_p): Sequential(
          (0): Conv1d(3, 3, kernel_size=(1,), stride=(1,))
          (1): BatchNorm1d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv1d(3, 512, kernel_size=(1,), stride=(1,))
        )
        (linear_w): Sequential(
          (0): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1): ReLU(inplace=True)
          (2): Conv1d(512, 64, kernel_size=(1,), stride=(1,))
          (3): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (4): ReLU(inplace=True)
          (5): Conv1d(64, 64, kernel_size=(1,), stride=(1,))
        )
        (softmax): Softmax(dim=1)
      )
      (bn2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (linear3): Linear(in_features=512, out_features=512, bias=False)
      (bn3): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (dec5): Sequential(
    (0): TransitionUp(
      (linear1): Sequential(
        (0): Linear(in_features=1024, out_features=512, bias=True)
        (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
      (linear2): Sequential(
        (0): Linear(in_features=512, out_features=512, bias=True)
        (1): ReLU(inplace=True)
      )
    )
    (1): PointTransformerBlock(
      (linear1): Linear(in_features=512, out_features=512, bias=False)
      (bn1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (transformer2): PointTransformerLayer(
        (linear_q): Linear(in_features=512, out_features=512, bias=True)
        (linear_k): Linear(in_features=512, out_features=512, bias=True)
        (linear_v): Linear(in_features=512, out_features=512, bias=True)
        (linear_p): Sequential(
          (0): Conv1d(3, 3, kernel_size=(1,), stride=(1,))
          (1): BatchNorm1d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv1d(3, 512, kernel_size=(1,), stride=(1,))
        )
        (linear_w): Sequential(
          (0): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1): ReLU(inplace=True)
          (2): Conv1d(512, 64, kernel_size=(1,), stride=(1,))
          (3): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (4): ReLU(inplace=True)
          (5): Conv1d(64, 64, kernel_size=(1,), stride=(1,))
        )
        (softmax): Softmax(dim=1)
      )
      (bn2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (linear3): Linear(in_features=512, out_features=512, bias=False)
      (bn3): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (rt5): RetroTransformer(
    (mlp_bottleneck): Sequential(
      (0): Linear(in_features=512, out_features=32, bias=False)
      (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Linear(in_features=32, out_features=32, bias=False)
    )
    (gav): CrossAttention(
      (linear_q): Linear(in_features=32, out_features=32, bias=True)
      (linear_k): Linear(in_features=32, out_features=32, bias=True)
      (linear_v): Linear(in_features=32, out_features=32, bias=True)
      (linear_p): Sequential(
        (0): Conv1d(3, 3, kernel_size=(1,), stride=(1,))
        (1): BatchNorm1d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
        (3): Conv1d(3, 2, kernel_size=(1,), stride=(1,))
      )
      (softmax): Softmax(dim=1)
    )
    (mlp_out): Sequential(
      (0): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): ReLU(inplace=True)
      (2): Linear(in_features=32, out_features=32, bias=False)
      (3): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (linear_info_curr): Linear(in_features=512, out_features=32, bias=True)
    (mlp_gate): Sequential(
      (0): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): Linear(in_features=32, out_features=32, bias=False)
      (2): Sigmoid()
    )
  )
  (cls5): Sequential(
    (0): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (1): ReLU()
    (2): Linear(in_features=32, out_features=13, bias=True)
  )
  (dec4): Sequential(
    (0): TransitionUp(
      (linear1): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
      (linear2): Sequential(
        (0): Linear(in_features=512, out_features=256, bias=True)
        (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
    )
    (1): PointTransformerBlock(
      (linear1): Linear(in_features=256, out_features=256, bias=False)
      (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (transformer2): PointTransformerLayer(
        (linear_q): Linear(in_features=256, out_features=256, bias=True)
        (linear_k): Linear(in_features=256, out_features=256, bias=True)
        (linear_v): Linear(in_features=256, out_features=256, bias=True)
        (linear_p): Sequential(
          (0): Conv1d(3, 3, kernel_size=(1,), stride=(1,))
          (1): BatchNorm1d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv1d(3, 256, kernel_size=(1,), stride=(1,))
        )
        (linear_w): Sequential(
          (0): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1): ReLU(inplace=True)
          (2): Conv1d(256, 32, kernel_size=(1,), stride=(1,))
          (3): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (4): ReLU(inplace=True)
          (5): Conv1d(32, 32, kernel_size=(1,), stride=(1,))
        )
        (softmax): Softmax(dim=1)
      )
      (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (linear3): Linear(in_features=256, out_features=256, bias=False)
      (bn3): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (rt4): RetroTransformer(
    (mlp_bottleneck): Sequential(
      (0): Linear(in_features=256, out_features=32, bias=False)
      (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Linear(in_features=32, out_features=32, bias=False)
    )
    (gav): CrossAttention(
      (linear_q): Linear(in_features=32, out_features=32, bias=True)
      (linear_k): Linear(in_features=32, out_features=32, bias=True)
      (linear_v): Linear(in_features=32, out_features=32, bias=True)
      (linear_p): Sequential(
        (0): Conv1d(3, 3, kernel_size=(1,), stride=(1,))
        (1): BatchNorm1d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
        (3): Conv1d(3, 2, kernel_size=(1,), stride=(1,))
      )
      (softmax): Softmax(dim=1)
    )
    (mlp_out): Sequential(
      (0): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): ReLU(inplace=True)
      (2): Linear(in_features=32, out_features=32, bias=False)
      (3): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (linear_info_curr): Linear(in_features=256, out_features=32, bias=True)
    (mlp_gate): Sequential(
      (0): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): Linear(in_features=32, out_features=32, bias=False)
      (2): Sigmoid()
    )
  )
  (cls4): Sequential(
    (0): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (1): ReLU()
    (2): Linear(in_features=32, out_features=13, bias=True)
  )
  (dec3): Sequential(
    (0): TransitionUp(
      (linear1): Sequential(
        (0): Linear(in_features=128, out_features=128, bias=True)
        (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
      (linear2): Sequential(
        (0): Linear(in_features=256, out_features=128, bias=True)
        (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
    )
    (1): PointTransformerBlock(
      (linear1): Linear(in_features=128, out_features=128, bias=False)
      (bn1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (transformer2): PointTransformerLayer(
        (linear_q): Linear(in_features=128, out_features=128, bias=True)
        (linear_k): Linear(in_features=128, out_features=128, bias=True)
        (linear_v): Linear(in_features=128, out_features=128, bias=True)
        (linear_p): Sequential(
          (0): Conv1d(3, 3, kernel_size=(1,), stride=(1,))
          (1): BatchNorm1d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv1d(3, 128, kernel_size=(1,), stride=(1,))
        )
        (linear_w): Sequential(
          (0): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1): ReLU(inplace=True)
          (2): Conv1d(128, 16, kernel_size=(1,), stride=(1,))
          (3): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (4): ReLU(inplace=True)
          (5): Conv1d(16, 16, kernel_size=(1,), stride=(1,))
        )
        (softmax): Softmax(dim=1)
      )
      (bn2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (linear3): Linear(in_features=128, out_features=128, bias=False)
      (bn3): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (rt3): RetroTransformer(
    (mlp_bottleneck): Sequential(
      (0): Linear(in_features=128, out_features=32, bias=False)
      (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Linear(in_features=32, out_features=32, bias=False)
    )
    (gav): CrossAttention(
      (linear_q): Linear(in_features=32, out_features=32, bias=True)
      (linear_k): Linear(in_features=32, out_features=32, bias=True)
      (linear_v): Linear(in_features=32, out_features=32, bias=True)
      (linear_p): Sequential(
        (0): Conv1d(3, 3, kernel_size=(1,), stride=(1,))
        (1): BatchNorm1d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
        (3): Conv1d(3, 2, kernel_size=(1,), stride=(1,))
      )
      (softmax): Softmax(dim=1)
    )
    (mlp_out): Sequential(
      (0): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): ReLU(inplace=True)
      (2): Linear(in_features=32, out_features=32, bias=False)
      (3): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (linear_info_curr): Linear(in_features=128, out_features=32, bias=True)
    (mlp_gate): Sequential(
      (0): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): Linear(in_features=32, out_features=32, bias=False)
      (2): Sigmoid()
    )
  )
  (cls3): Sequential(
    (0): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (1): ReLU()
    (2): Linear(in_features=32, out_features=13, bias=True)
  )
  (dec2): Sequential(
    (0): TransitionUp(
      (linear1): Sequential(
        (0): Linear(in_features=64, out_features=64, bias=True)
        (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
      (linear2): Sequential(
        (0): Linear(in_features=128, out_features=64, bias=True)
        (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
    )
    (1): PointTransformerBlock(
      (linear1): Linear(in_features=64, out_features=64, bias=False)
      (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (transformer2): PointTransformerLayer(
        (linear_q): Linear(in_features=64, out_features=64, bias=True)
        (linear_k): Linear(in_features=64, out_features=64, bias=True)
        (linear_v): Linear(in_features=64, out_features=64, bias=True)
        (linear_p): Sequential(
          (0): Conv1d(3, 3, kernel_size=(1,), stride=(1,))
          (1): BatchNorm1d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv1d(3, 64, kernel_size=(1,), stride=(1,))
        )
        (linear_w): Sequential(
          (0): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1): ReLU(inplace=True)
          (2): Conv1d(64, 8, kernel_size=(1,), stride=(1,))
          (3): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (4): ReLU(inplace=True)
          (5): Conv1d(8, 8, kernel_size=(1,), stride=(1,))
        )
        (softmax): Softmax(dim=1)
      )
      (bn2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (linear3): Linear(in_features=64, out_features=64, bias=False)
      (bn3): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (rt2): RetroTransformer(
    (mlp_bottleneck): Sequential(
      (0): Linear(in_features=64, out_features=32, bias=False)
      (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Linear(in_features=32, out_features=32, bias=False)
    )
    (gav): CrossAttention(
      (linear_q): Linear(in_features=32, out_features=32, bias=True)
      (linear_k): Linear(in_features=32, out_features=32, bias=True)
      (linear_v): Linear(in_features=32, out_features=32, bias=True)
      (linear_p): Sequential(
        (0): Conv1d(3, 3, kernel_size=(1,), stride=(1,))
        (1): BatchNorm1d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
        (3): Conv1d(3, 2, kernel_size=(1,), stride=(1,))
      )
      (softmax): Softmax(dim=1)
    )
    (mlp_out): Sequential(
      (0): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): ReLU(inplace=True)
      (2): Linear(in_features=32, out_features=32, bias=False)
      (3): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (linear_info_curr): Linear(in_features=64, out_features=32, bias=True)
    (mlp_gate): Sequential(
      (0): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): Linear(in_features=32, out_features=32, bias=False)
      (2): Sigmoid()
    )
  )
  (cls2): Sequential(
    (0): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (1): ReLU()
    (2): Linear(in_features=32, out_features=13, bias=True)
  )
  (dec1): Sequential(
    (0): TransitionUp(
      (linear1): Sequential(
        (0): Linear(in_features=32, out_features=32, bias=True)
        (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
      (linear2): Sequential(
        (0): Linear(in_features=64, out_features=32, bias=True)
        (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
      )
    )
    (1): PointTransformerBlock(
      (linear1): Linear(in_features=32, out_features=32, bias=False)
      (bn1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (transformer2): PointTransformerLayer(
        (linear_q): Linear(in_features=32, out_features=32, bias=True)
        (linear_k): Linear(in_features=32, out_features=32, bias=True)
        (linear_v): Linear(in_features=32, out_features=32, bias=True)
        (linear_p): Sequential(
          (0): Conv1d(3, 3, kernel_size=(1,), stride=(1,))
          (1): BatchNorm1d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv1d(3, 32, kernel_size=(1,), stride=(1,))
        )
        (linear_w): Sequential(
          (0): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1): ReLU(inplace=True)
          (2): Conv1d(32, 4, kernel_size=(1,), stride=(1,))
          (3): BatchNorm1d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (4): ReLU(inplace=True)
          (5): Conv1d(4, 4, kernel_size=(1,), stride=(1,))
        )
        (softmax): Softmax(dim=1)
      )
      (bn2): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (linear3): Linear(in_features=32, out_features=32, bias=False)
      (bn3): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (rt1): RetroTransformer(
    (mlp_bottleneck): Sequential(
      (0): Linear(in_features=32, out_features=32, bias=False)
      (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Linear(in_features=32, out_features=32, bias=False)
    )
    (gav): CrossAttention(
      (linear_q): Linear(in_features=32, out_features=32, bias=True)
      (linear_k): Linear(in_features=32, out_features=32, bias=True)
      (linear_v): Linear(in_features=32, out_features=32, bias=True)
      (linear_p): Sequential(
        (0): Conv1d(3, 3, kernel_size=(1,), stride=(1,))
        (1): BatchNorm1d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
        (3): Conv1d(3, 2, kernel_size=(1,), stride=(1,))
      )
      (softmax): Softmax(dim=1)
    )
    (mlp_out): Sequential(
      (0): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): ReLU(inplace=True)
      (2): Linear(in_features=32, out_features=32, bias=False)
      (3): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (linear_info_curr): Linear(in_features=32, out_features=32, bias=True)
    (mlp_gate): Sequential(
      (0): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): Linear(in_features=32, out_features=32, bias=False)
      (2): Sigmoid()
    )
  )
  (cls1): Sequential(
    (0): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (1): ReLU()
    (2): Linear(in_features=32, out_features=13, bias=True)
  )
)
[2023-11-05 13:28:39,103 INFO test.py line 73 3968969] => loading checkpoint './checkpoints/model_best.pth'
[2023-11-05 13:31:13,644 INFO test.py line 81 3968969] => loaded checkpoint './checkpoints/model_best.pth' (epoch 66)
[2023-11-05 13:31:13,644 INFO test.py line 125 3968969] >>>>>>>>>>>>>>>> Start Evaluation >>>>>>>>>>>>>>>>
[2023-11-05 13:31:15,152 INFO test.py line 149 3968969] 1/68: 1/65/47358, XPArea_5_WC_1
[2023-11-05 13:31:15,171 INFO test.py line 149 3968969] 1/68: 2/65/47358, XPArea_5_WC_1
[2023-11-05 13:31:15,190 INFO test.py line 149 3968969] 1/68: 3/65/47358, XPArea_5_WC_1
[2023-11-05 13:31:15,210 INFO test.py line 149 3968969] 1/68: 4/65/47358, XPArea_5_WC_1
[2023-11-05 13:31:15,231 INFO test.py line 149 3968969] 1/68: 5/65/47358, XPArea_5_WC_1
[2023-11-05 13:31:15,251 INFO test.py line 149 3968969] 1/68: 6/65/47358, XPArea_5_WC_1
[2023-11-05 13:31:15,263 INFO test.py line 149 3968969] 1/68: 7/65/47358, XPArea_5_WC_1
[2023-11-05 13:31:15,286 INFO test.py line 149 3968969] 1/68: 8/65/47358, XPArea_5_WC_1
[2023-11-05 13:31:15,307 INFO test.py line 149 3968969] 1/68: 9/65/47358, XPArea_5_WC_1
[2023-11-05 13:31:15,326 INFO test.py line 149 3968969] 1/68: 10/65/47358, XPArea_5_WC_1
[2023-11-05 13:31:15,339 INFO test.py line 149 3968969] 1/68: 11/65/47358, XPArea_5_WC_1
[2023-11-05 13:31:15,351 INFO test.py line 149 3968969] 1/68: 12/65/47358, XPArea_5_WC_1
[2023-11-05 13:31:15,364 INFO test.py line 149 3968969] 1/68: 13/65/47358, XPArea_5_WC_1
[2023-11-05 13:31:15,376 INFO test.py line 149 3968969] 1/68: 14/65/47358, XPArea_5_WC_1
[2023-11-05 13:31:15,388 INFO test.py line 149 3968969] 1/68: 15/65/47358, XPArea_5_WC_1
[2023-11-05 13:31:15,400 INFO test.py line 149 3968969] 1/68: 16/65/47358, XPArea_5_WC_1
[2023-11-05 13:31:15,413 INFO test.py line 149 3968969] 1/68: 17/65/47358, XPArea_5_WC_1
[2023-11-05 13:31:15,425 INFO test.py line 149 3968969] 1/68: 18/65/47358, XPArea_5_WC_1
[2023-11-05 13:31:15,437 INFO test.py line 149 3968969] 1/68: 19/65/47358, XPArea_5_WC_1
[2023-11-05 13:31:15,449 INFO test.py line 149 3968969] 1/68: 20/65/47358, XPArea_5_WC_1
[2023-11-05 13:31:15,461 INFO test.py line 149 3968969] 1/68: 21/65/47358, XPArea_5_WC_1
[2023-11-05 13:31:15,474 INFO test.py line 149 3968969] 1/68: 22/65/47358, XPArea_5_WC_1
[2023-11-05 13:31:15,486 INFO test.py line 149 3968969] 1/68: 23/65/47358, XPArea_5_WC_1
[2023-11-05 13:31:15,498 INFO test.py line 149 3968969] 1/68: 24/65/47358, XPArea_5_WC_1
[2023-11-05 13:31:15,511 INFO test.py line 149 3968969] 1/68: 25/65/47358, XPArea_5_WC_1
[2023-11-05 13:31:15,523 INFO test.py line 149 3968969] 1/68: 26/65/47358, XPArea_5_WC_1
[2023-11-05 13:31:15,535 INFO test.py line 149 3968969] 1/68: 27/65/47358, XPArea_5_WC_1
[2023-11-05 13:31:15,547 INFO test.py line 149 3968969] 1/68: 28/65/47358, XPArea_5_WC_1
[2023-11-05 13:31:15,558 INFO test.py line 149 3968969] 1/68: 29/65/47358, XPArea_5_WC_1
[2023-11-05 13:31:15,572 INFO test.py line 149 3968969] 1/68: 30/65/47358, XPArea_5_WC_1
[2023-11-05 13:31:15,583 INFO test.py line 149 3968969] 1/68: 31/65/47358, XPArea_5_WC_1
[2023-11-05 13:31:15,595 INFO test.py line 149 3968969] 1/68: 32/65/47358, XPArea_5_WC_1
[2023-11-05 13:31:15,606 INFO test.py line 149 3968969] 1/68: 33/65/47358, XPArea_5_WC_1
[2023-11-05 13:31:15,618 INFO test.py line 149 3968969] 1/68: 34/65/47358, XPArea_5_WC_1
[2023-11-05 13:31:15,629 INFO test.py line 149 3968969] 1/68: 35/65/47358, XPArea_5_WC_1
[2023-11-05 13:31:15,640 INFO test.py line 149 3968969] 1/68: 36/65/47358, XPArea_5_WC_1
[2023-11-05 13:31:15,651 INFO test.py line 149 3968969] 1/68: 37/65/47358, XPArea_5_WC_1
[2023-11-05 13:31:15,662 INFO test.py line 149 3968969] 1/68: 38/65/47358, XPArea_5_WC_1
[2023-11-05 13:31:15,673 INFO test.py line 149 3968969] 1/68: 39/65/47358, XPArea_5_WC_1
[2023-11-05 13:31:15,683 INFO test.py line 149 3968969] 1/68: 40/65/47358, XPArea_5_WC_1
[2023-11-05 13:31:15,694 INFO test.py line 149 3968969] 1/68: 41/65/47358, XPArea_5_WC_1
[2023-11-05 13:31:15,705 INFO test.py line 149 3968969] 1/68: 42/65/47358, XPArea_5_WC_1
[2023-11-05 13:31:15,716 INFO test.py line 149 3968969] 1/68: 43/65/47358, XPArea_5_WC_1
[2023-11-05 13:31:15,727 INFO test.py line 149 3968969] 1/68: 44/65/47358, XPArea_5_WC_1
[2023-11-05 13:31:15,738 INFO test.py line 149 3968969] 1/68: 45/65/47358, XPArea_5_WC_1
[2023-11-05 13:31:15,749 INFO test.py line 149 3968969] 1/68: 46/65/47358, XPArea_5_WC_1
[2023-11-05 13:31:15,760 INFO test.py line 149 3968969] 1/68: 47/65/47358, XPArea_5_WC_1
[2023-11-05 13:31:15,772 INFO test.py line 149 3968969] 1/68: 48/65/47358, XPArea_5_WC_1
[2023-11-05 13:31:15,784 INFO test.py line 149 3968969] 1/68: 49/65/47358, XPArea_5_WC_1
[2023-11-05 13:31:15,795 INFO test.py line 149 3968969] 1/68: 50/65/47358, XPArea_5_WC_1
[2023-11-05 13:31:15,806 INFO test.py line 149 3968969] 1/68: 51/65/47358, XPArea_5_WC_1
[2023-11-05 13:31:15,818 INFO test.py line 149 3968969] 1/68: 52/65/47358, XPArea_5_WC_1
[2023-11-05 13:31:15,830 INFO test.py line 149 3968969] 1/68: 53/65/47358, XPArea_5_WC_1
[2023-11-05 13:31:15,841 INFO test.py line 149 3968969] 1/68: 54/65/47358, XPArea_5_WC_1
[2023-11-05 13:31:15,853 INFO test.py line 149 3968969] 1/68: 55/65/47358, XPArea_5_WC_1
[2023-11-05 13:31:15,865 INFO test.py line 149 3968969] 1/68: 56/65/47358, XPArea_5_WC_1
[2023-11-05 13:31:15,876 INFO test.py line 149 3968969] 1/68: 57/65/47358, XPArea_5_WC_1
[2023-11-05 13:31:15,888 INFO test.py line 149 3968969] 1/68: 58/65/47358, XPArea_5_WC_1
[2023-11-05 13:31:15,899 INFO test.py line 149 3968969] 1/68: 59/65/47358, XPArea_5_WC_1
[2023-11-05 13:31:15,910 INFO test.py line 149 3968969] 1/68: 60/65/47358, XPArea_5_WC_1
[2023-11-05 13:31:15,920 INFO test.py line 149 3968969] 1/68: 61/65/47358, XPArea_5_WC_1
[2023-11-05 13:31:15,932 INFO test.py line 149 3968969] 1/68: 62/65/47358, XPArea_5_WC_1
[2023-11-05 13:31:15,943 INFO test.py line 149 3968969] 1/68: 63/65/47358, XPArea_5_WC_1
[2023-11-05 13:31:15,955 INFO test.py line 149 3968969] 1/68: 64/65/47358, XPArea_5_WC_1
[2023-11-05 13:31:15,966 INFO test.py line 149 3968969] 1/68: 65/65/47358, XPArea_5_WC_1
